What the plan is

Optimizers
- General Introduction: https://arxiv.org/pdf/1609.04747
- SGD: Nestorov, Momentum, L2, and Damp: 
    - https://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf
- RMSProp
    - https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf
- AdaGrad:
    - https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf
- Adam
    - https://arxiv.org/pdf/1412.6980
- AdamW
    - https://arxiv.org/pdf/1711.05101
- LAMB
    - https://arxiv.org/pdf/1904.00962


Normalization
- Batch Normalization: 
    - https://arxiv.org/abs/1502.03167
- Layer Normalization:
    - https://arxiv.org/abs/1607.06450
- Dropout
    - https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf





TODO Tommorow
- Implement Softmax
- Implement Log Softmax
- Implement Clipping into tensor
- Implement CrossEntropy Loss
- Implement BCE
- Implement abs to the tensor class
- Implement L1 loss
- Test Losss with Sklearn
- Need to implement clipping