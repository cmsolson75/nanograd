{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import Tensor\n",
    "import numpy as np\n",
    "import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.MLP()\n",
    "\n",
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0],\n",
    "]\n",
    "\n",
    "ys = [[1.0], [-1.0], [-1.0], [1.0]]\n",
    "\n",
    "y_hat = net(Tensor(xs))\n",
    "y_real = Tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.        ]\n",
       " [0.16429852]\n",
       " [0.        ]\n",
       " [0.        ]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_real.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numba import jit\n",
    "\n",
    "# @jit(nopython=True)\n",
    "# def loss_fn(y_hat, y_real):\n",
    "#     loss = ((y_hat - y_real)**2).sum() / y_real.shape[0]\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.6143259\n",
      "200 0.1426598\n",
      "400 0.0062944293\n",
      "600 0.0005949866\n",
      "800 0.00014812025\n",
      "1000 5.059447e-05\n",
      "1200 1.8746636e-05\n",
      "1400 7.2093917e-06\n",
      "1600 2.8343127e-06\n",
      "1800 1.1295688e-06\n",
      "2000 4.5412784e-07\n",
      "2200 1.8339904e-07\n",
      "2400 7.451002e-08\n",
      "2600 3.0498935e-08\n",
      "2800 1.2621408e-08\n",
      "3000 5.1890323e-09\n",
      "3200 2.144045e-09\n",
      "3400 1.037038e-09\n",
      "3600 5.4879923e-10\n",
      "3800 3.51136e-10\n",
      "4000 2.5344793e-10\n",
      "4200 1.8632562e-10\n",
      "4400 1.7123458e-10\n",
      "4600 1.5911628e-10\n",
      "4800 1.5126478e-10\n",
      "5000 1.4310686e-10\n",
      "5200 1.3323387e-10\n",
      "5400 1.2490275e-10\n",
      "5600 1.1798562e-10\n",
      "5800 1.1335288e-10\n",
      "6000 1.0630252e-10\n",
      "6200 1.01000985e-10\n",
      "6400 9.64544e-11\n",
      "6600 9.329515e-11\n",
      "6800 8.602541e-11\n",
      "7000 8.1400664e-11\n",
      "7200 7.76712e-11\n",
      "7400 7.45537e-11\n",
      "7600 7.224177e-11\n",
      "7800 6.827605e-11\n",
      "8000 6.636913e-11\n",
      "8200 6.445777e-11\n",
      "8400 6.2155614e-11\n",
      "8600 6.1360694e-11\n",
      "8800 6.0044414e-11\n",
      "9000 5.853984e-11\n",
      "9200 5.82725e-11\n",
      "9400 5.6400218e-11\n",
      "9600 5.411227e-11\n",
      "9800 5.3632654e-11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[ 1.0000103]\n",
       " [-0.9999937]\n",
       " [-1.0000002]\n",
       " [ 0.999992 ]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0],\n",
    "]\n",
    "\n",
    "ys = [[1.0], \n",
    "      [-1.0], \n",
    "      [-1.0], \n",
    "      [1.0]]\n",
    "\n",
    "\n",
    "net = nn.MLP()\n",
    "\n",
    "x = Tensor(xs)\n",
    "y_real = Tensor(ys)\n",
    "\n",
    "for k in range(10000):\n",
    "    net.zero_grad()  # Use the zero_grad method to properly zero out gradients.\n",
    "    y_pred = net(x)\n",
    "    loss = ((y_pred - y_real)**2).sum() / y_real.shape[0]\n",
    "#     loss = loss_fn(y_pred, y_real)\n",
    "    loss.backward()\n",
    "    \n",
    "    for p in net.parameters():\n",
    "        if p.grad is not None:\n",
    "            p.data -= 0.005 * p.grad  # Apply the learning rate.\n",
    "    \n",
    "    if k % 200 == 0:\n",
    "        print(k, loss.data)\n",
    "    \n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.10926643  0.34880325  0.34553602  0.00214646]\n",
      " [-0.03642214 -0.02780569 -0.02754523 -0.00071549]\n",
      " [ 0.01821107  0.12763979  0.12644419  0.00035774]]\n",
      "[[0. 0. 0. 0.]]\n",
      "[[0.         0.00267239 0.         0.        ]\n",
      " [0.         0.18684575 0.         0.        ]\n",
      " [0.         0.6182679  0.         0.        ]\n",
      " [0.         0.38165408 0.         0.        ]]\n",
      "[[0. 0. 0. 0.]]\n",
      "[[0.       ]\n",
      " [4.3699355]\n",
      " [0.       ]\n",
      " [0.       ]]\n",
      "[[0.]]\n"
     ]
    }
   ],
   "source": [
    "for p in net.parameters():\n",
    "    print(p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.random.randn(4, 4)@np.random.randn(4, 1) + np.zeros((1, 1))\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (1,1) doesn't match the broadcast shape (4,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-37d1999ecf13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: non-broadcastable output operand with shape (1,1) doesn't match the broadcast shape (4,1)"
     ]
    }
   ],
   "source": [
    "c = np.zeros((1, 1))\n",
    "c += np.random.randn(4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.]\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "(4, 1)\n",
      "[[-2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [-2.]]\n",
      "None\n",
      "None\n",
      "None\n",
      "[[-2.]\n",
      " [ 2.]\n",
      " [ 2.]\n",
      " [-2.]]\n",
      "[[-0.6084045 ]\n",
      " [-0.9584013 ]\n",
      " [-0.06329735]\n",
      " [-1.1529789 ]]\n",
      "(4, 1)\n",
      "[0.]\n",
      "(1,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (1,) doesn't match the broadcast shape (4,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-52a0569421b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Developer/dl_work/Learning/Explore/NanoGrad/engine.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0mtopo_sorted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_topological_sort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopo_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Developer/dl_work/Learning/Explore/NanoGrad/engine.py\u001b[0m in \u001b[0;36m_compute_gradients\u001b[0;34m(self, topo_sorted)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopo_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Developer/dl_work/Learning/Explore/NanoGrad/engine.py\u001b[0m in \u001b[0;36m_backward\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: non-broadcastable output operand with shape (1,) doesn't match the broadcast shape (4,1)"
     ]
    }
   ],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_reduce.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims([1, 1], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4, 2)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.random.randn(1, 4, 2) # match the input dims and you are good.\n",
    "c = np.random.randn(4, 2)\n",
    "\n",
    "(b * c).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4, 2)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.randn(1, 4, 2)\n",
    "\n",
    "a += (b * c)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(1, 4, 2).sum(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_reduce = np.random.randn(4, 1)\n",
    "grad = np.ones_like(to_reduce)\n",
    "# to_reduce.sum(0)\n",
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix sum you just need to match the shape of the self.data coming in, I think. More accuratly you need to match the shape of the transformation maybe: idk. Need to work with sum more. numpy and torch has sum play with them.\n",
    "\n",
    "\n",
    "Need to get the dims to match correctly, this will take some experimentation: and deep thought. \n",
    "\n",
    "Foundationaly need to understand reduction ops. This is what is hard for me I think\n",
    "\n",
    "\n",
    "What do I know\n",
    "- Its still add so der = 1\n",
    "- Its shape needs to broadcast in the der, so if the shape is 4, 1 on the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.97902001],\n",
       "       [-2.22508662],\n",
       "       [-1.67047627],\n",
       "       [-0.2965853 ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.randn(4, 1)\n",
    "b = np.random.randn(4, 1)\n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for p in net.parameters():\n",
    "    print(p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[ 1.]\n",
       " [-1.]\n",
       " [-1.]\n",
       " [ 1.]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2, [[-1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [-1.]]}\n"
     ]
    }
   ],
   "source": [
    "for i in loss._prev:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.07063]\n",
      "[[1.       ]\n",
      " [1.       ]\n",
      " [1.0730093]\n",
      " [0.9976208]]\n",
      "[[-1.       ]\n",
      " [ 1.       ]\n",
      " [ 1.0358616]\n",
      " [-0.9988097]]\n",
      "[[0.        ]\n",
      " [0.        ]\n",
      " [0.03586162]\n",
      " [0.00119028]]\n",
      "[[ 0.0000000e+00]\n",
      " [-1.6198169e+00]\n",
      " [ 3.5861615e-02]\n",
      " [ 1.1902830e-03]]\n",
      "[[0.]]\n",
      "[[ 0.0000000e+00]\n",
      " [-1.6198169e+00]\n",
      " [ 3.5861615e-02]\n",
      " [ 1.1902830e-03]]\n",
      "[[0.         0.         0.         0.        ]\n",
      " [0.         2.420442   0.         0.        ]\n",
      " [0.         0.         0.3001468  0.        ]\n",
      " [0.         0.         0.00996217 0.        ]]\n",
      "[[ 0.          0.          0.          0.        ]\n",
      " [-2.5953069   2.420442   -3.161884   -0.6409065 ]\n",
      " [-0.43346208 -0.06011634  0.3001468  -0.15714166]\n",
      " [-0.28839874 -0.12067308  0.00996217 -0.29038864]]\n",
      "[[ 0.          0.          0.          0.        ]\n",
      " [-2.5953069   2.420442   -3.161884   -0.6409065 ]\n",
      " [-0.43346208 -0.06011634  0.3001468  -0.15714166]\n",
      " [-0.28839874 -0.12067308  0.00996217 -0.29038864]]\n",
      "[[0.         0.         0.         0.        ]\n",
      " [0.         0.         0.30719423 3.4540505 ]\n",
      " [0.         0.         0.41605794 0.        ]\n",
      " [0.28141475 0.         0.         0.        ]]\n",
      "[[-0.1343242  -1.3368311  -3.352221   -1.8083171 ]\n",
      " [-2.3013084  -1.0301435   0.30719423  3.4540505 ]\n",
      " [-0.9990918  -0.34954017  0.41605794 -0.29224193]\n",
      " [ 0.28141475 -0.5945108  -1.9520645  -0.5673622 ]]\n",
      "[[-0.1343242  -1.3368311  -3.352221   -1.8083171 ]\n",
      " [-2.3013084  -1.0301435   0.30719423  3.4540505 ]\n",
      " [-0.9990918  -0.34954017  0.41605794 -0.29224193]\n",
      " [ 0.28141475 -0.5945108  -1.9520645  -0.5673622 ]]\n",
      "[[ 2.   3.  -1. ]\n",
      " [ 3.  -1.   0.5]\n",
      " [ 0.5  1.   1. ]\n",
      " [ 1.   1.  -1. ]]\n",
      "[[-0.6038762  -0.40346107 -0.27170056  0.7627015 ]\n",
      " [ 0.0940686  -0.16942966 -0.5642279  -1.0018282 ]\n",
      " [-0.79122233  0.02162005  1.1161361   0.3282355 ]]\n",
      "[[0. 0. 0. 0.]]\n",
      "[[-1.0248175  -0.42880863  0.03540032 -1.0318885 ]\n",
      " [-0.50718474  1.1159618  -0.16245256  0.22184539]\n",
      " [-1.041831   -0.14449032  0.7214062  -0.37769175]\n",
      " [-0.65872294  0.7136053  -0.9795733  -0.15196124]]\n",
      "[[0. 0. 0. 0.]]\n",
      "[[-1.1096704 ]\n",
      " [-0.66922355]\n",
      " [ 0.11948026]\n",
      " [-0.30751005]]\n",
      "[[-1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [-1.]]\n",
      "-1.0\n",
      "[[ 1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [ 1.]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 4 into shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-52a0569421b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Developer/dl_work/Learning/Explore/NanoGrad/engine.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Developer/dl_work/Learning/Explore/NanoGrad/engine.py\u001b[0m in \u001b[0;36m_backward\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                     \u001b[0mshape_diff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape_diff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape_diff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 4 into shape (1,)"
     ]
    }
   ],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "engine.Tensor"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7fba2f775eb0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.1243251  -0.9172921   1.182684   -1.0130153 ]\n",
      " [ 0.0288723   1.3303456  -0.11303287  0.24445443]\n",
      " [-1.1465033   0.86183876  0.4577391  -0.36788297]]\n",
      "[[0. 0. 0. 0.]]\n",
      "[[ 0.28105614 -0.3855348   0.7029505   0.34809703]\n",
      " [-0.41025335 -0.5370706  -1.1471525  -0.20046443]\n",
      " [-0.37782416  0.5260139  -0.13071114  0.3492137 ]\n",
      " [-1.03824    -0.7071723  -0.5668519   0.07607934]]\n",
      "[[0. 0. 0. 0.]]\n",
      "[[ 1.0335304 ]\n",
      " [-0.10125775]\n",
      " [ 0.24842171]\n",
      " [ 0.9971799 ]]\n",
      "[[0.]]\n"
     ]
    }
   ],
   "source": [
    "for param in net.parameters():\n",
    "    if param.requires_grad:\n",
    "#         param.data -= self.lr * param.grad\n",
    "        print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Tensor([[1, 2]])\n",
    "print(x.shape)\n",
    "l1(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'w': [[ 0.95477086  0.8455835   0.98341113  0.7739111 ]\n",
      " [ 0.9920584   0.34109655 -0.22134824  1.0318031 ]\n",
      " [ 1.3880258  -0.6933539  -0.00778713  0.9643549 ]], 'b': [0. 0. 0. 0.]}\n",
      "{'w': [[-1.018422    0.16992508 -0.06235736  0.9246361 ]\n",
      " [-1.1396269   0.2504219  -0.78362983 -0.10598446]\n",
      " [-1.130974    0.29714298 -0.39758295  0.66354245]\n",
      " [ 1.2067316   1.0690259   0.1981175   0.08408989]], 'b': [0. 0. 0. 0.]}\n",
      "{'w': [[ 0.07227279]\n",
      " [-0.25291726]\n",
      " [-0.7082981 ]\n",
      " [ 0.64171904]], 'b': [0.]}\n"
     ]
    }
   ],
   "source": [
    "for p in net.parameters().values():\n",
    "    print(p.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = nn.Linear(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w': [[-0.5850506   1.6288409   0.39408326]\n",
       "  [ 1.1982913  -1.0406932  -0.23052394]],\n",
       " 'b': [0. 0. 0.]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = Tensor([[4.0, 2.0]], requires_grad=True)\n",
    "w1 = Tensor([[3.0, 4.0]], requires_grad=True)\n",
    "\n",
    "\n",
    "\n",
    "b1 = Tensor([[7.0]], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0],\n",
    "]\n",
    "\n",
    "ys = [1.0, -1.0, -1.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array([2, 3], np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_1 = [[1, 2]]\n",
    "input_2 = np.array([[1, 2]])\n",
    "\n",
    "Tensor(input_1) is Tensor(input_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tensor.kaiming_uniform(3, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.56948177,  1.47046888, -0.70346975],\n",
       "       [-2.12550729,  1.64349633, -0.18920115],\n",
       "       [ 0.06239465,  0.3143566 ,  0.88756801]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.normal(0, 1, (3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaiming_uniform(fan_in, fan_out):\n",
    "    limit = np.sqrt(6 / fan_in)\n",
    "    return np.random.uniform(-limit, limit, (fan_out, fan_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model init\n",
    "def xavier_init(shape: tuple):\n",
    "    in_dims, out_dims = shape\n",
    "    variance = 2 / (in_dims + out_dims)\n",
    "    return np.random.normal(0, np.sqrt(variance), shape)\n",
    "\n",
    "\n",
    "def init_layer(n_in, n_out):\n",
    "    w = np.random.normal(0, 1, (n_in, n_out))\n",
    "    b = np.random.normal(0, 1, (n_out))\n",
    "    return Tensor(w, requires_grad=True), Tensor(b, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-1.2053158  2.538495  -1.2465813  2.213791 ]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Tensor(np.random.normal(0, 1, (1, 3)), requires_grad=True)\n",
    "\n",
    "w, b = init_layer(3, 4)\n",
    "\n",
    "x@w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "(3, 4), (1, 3): Incompatible shapes for matrix multiplication.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-c6db6cf4b79d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Developer/dl_work/Learning/Explore/NanoGrad/engine.py\u001b[0m in \u001b[0;36m__matmul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: (3, 4), (1, 3): Incompatible shapes for matrix multiplication."
     ]
    }
   ],
   "source": [
    "w@x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def access_params(net_params, func):\n",
    "    for layers in net_params.values():\n",
    "        for p in layers.parameters().values():\n",
    "            func(p)\n",
    "        \n",
    "def zero_grad(params):\n",
    "    for p in params.data:\n",
    "        print(p)\n",
    "        p = np.zeros_like(p)\n",
    "        print(p)\n",
    "        \n",
    "def print_params(params):\n",
    "    print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.86006254 -0.30643904 -0.21797846  1.0343217 ]\n",
      " [-1.1683605   0.95028704 -0.40874273  0.4400779 ]\n",
      " [-0.14941764  0.5234418   0.82390934  0.97237396]]\n",
      "[[0. 0. 0. 0.]]\n",
      "[[ 1.0939686   0.61001337 -0.64165026  0.71326745]\n",
      " [-0.3159256  -0.44704282  1.1607931  -0.32719943]\n",
      " [-0.76023024  1.1756271  -0.33649942 -1.1185616 ]\n",
      " [ 0.74409056 -0.86787856 -0.07981426  0.07215461]]\n",
      "[[0. 0. 0. 0.]]\n",
      "[[ 0.83443314]\n",
      " [-0.18828385]\n",
      " [-0.06012312]\n",
      " [-0.46142915]]\n",
      "[[0.]]\n"
     ]
    }
   ],
   "source": [
    "access_params(net.parameters(), print_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.86006254 -0.30643904 -0.21797846  1.0343217 ]\n",
      "[0. 0. 0. 0.]\n",
      "[-1.1683605   0.95028704 -0.40874273  0.4400779 ]\n",
      "[0. 0. 0. 0.]\n",
      "[-0.14941764  0.5234418   0.82390934  0.97237396]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[ 1.0939686   0.61001337 -0.64165026  0.71326745]\n",
      "[0. 0. 0. 0.]\n",
      "[-0.3159256  -0.44704282  1.1607931  -0.32719943]\n",
      "[0. 0. 0. 0.]\n",
      "[-0.76023024  1.1756271  -0.33649942 -1.1185616 ]\n",
      "[0. 0. 0. 0.]\n",
      "[ 0.74409056 -0.86787856 -0.07981426  0.07215461]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[0.83443314]\n",
      "[0.]\n",
      "[-0.18828385]\n",
      "[0.]\n",
      "[-0.06012312]\n",
      "[0.]\n",
      "[-0.46142915]\n",
      "[0.]\n",
      "[0.]\n",
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "access_params(net.parameters(), zero_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(10000):\n",
    "    ypred = [n(x) for x in xs]\n",
    "    loss = sum([(y_hat - y_real)**2 for y_real, y_hat in zip(ys, ypred)])\n",
    "    \n",
    "    \n",
    "    for p in n.parameters():\n",
    "        p.grad = 0 # zero grad\n",
    "    loss.backward()\n",
    "    \n",
    "    for p in n.parameters():\n",
    "        p.data += -0.05 * p.grad\n",
    "    print(k, loss.data)\n",
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Tensor' has no attribute 'zero_init'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b09d1425f76a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Developer/dl_work/Learning/Explore/NanoGrad/nn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Developer/dl_work/Learning/Explore/NanoGrad/nn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_dims, out_dims)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkaiming_uniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'Tensor' has no attribute 'zero_init'"
     ]
    }
   ],
   "source": [
    "net = nn.MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test:\n",
    "    def __init__(self):\n",
    "        self.a1 = 'hi'\n",
    "        self.a2 = 'hihi'\n",
    "        \n",
    "    def parameters(self):\n",
    "        return vars(self)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_dict = t.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kwa:\n",
    "    def __init__(self, name=\"None\"):\n",
    "        self.name = name\n",
    "    \n",
    "    @classmethod\n",
    "    def make(cls, **kwargs):\n",
    "        return cls(**kwargs)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"KWA({self.name})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = Kwa.make(name=\"Cam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cam'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_tensor = np.random.randn(4, 1)\n",
    "other_tensor = np.random.randn(1, 1)\n",
    "\n",
    "grad_tensor = np.random.randn(1, 1)\n",
    "\n",
    "\n",
    "grad_self_shape = np.broadcast_to(other_tensor, self_tensor.shape).shape\n",
    "grad_self_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (1,1) doesn't match the broadcast shape (4,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-73bcc48736ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrad_tensor\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mgrad_self_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: non-broadcastable output operand with shape (1,1) doesn't match the broadcast shape (4,1)"
     ]
    }
   ],
   "source": [
    "grad_tensor += np.sum(self_tensor, axis=tuple(i for i in range(self_tensor.ndim) if grad_self_shape[i] == 1), keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.broadcast_to??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.42807348]]\n",
      "-0.4280734824847532\n"
     ]
    }
   ],
   "source": [
    "data = np.random.randn(1, 1)\n",
    "print(data)\n",
    "\n",
    "\n",
    "\n",
    "print(np.sum(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.4321468, -0.5955731]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(*(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function arange in module numpy:\n",
      "\n",
      "arange(...)\n",
      "    arange([start,] stop[, step,], dtype=None)\n",
      "    \n",
      "    Return evenly spaced values within a given interval.\n",
      "    \n",
      "    Values are generated within the half-open interval ``[start, stop)``\n",
      "    (in other words, the interval including `start` but excluding `stop`).\n",
      "    For integer arguments the function is equivalent to the Python built-in\n",
      "    `range` function, but returns an ndarray rather than a list.\n",
      "    \n",
      "    When using a non-integer step, such as 0.1, the results will often not\n",
      "    be consistent.  It is better to use `numpy.linspace` for these cases.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    start : number, optional\n",
      "        Start of interval.  The interval includes this value.  The default\n",
      "        start value is 0.\n",
      "    stop : number\n",
      "        End of interval.  The interval does not include this value, except\n",
      "        in some cases where `step` is not an integer and floating point\n",
      "        round-off affects the length of `out`.\n",
      "    step : number, optional\n",
      "        Spacing between values.  For any output `out`, this is the distance\n",
      "        between two adjacent values, ``out[i+1] - out[i]``.  The default\n",
      "        step size is 1.  If `step` is specified as a position argument,\n",
      "        `start` must also be given.\n",
      "    dtype : dtype\n",
      "        The type of the output array.  If `dtype` is not given, infer the data\n",
      "        type from the other input arguments.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    arange : ndarray\n",
      "        Array of evenly spaced values.\n",
      "    \n",
      "        For floating point arguments, the length of the result is\n",
      "        ``ceil((stop - start)/step)``.  Because of floating point overflow,\n",
      "        this rule may result in the last element of `out` being greater\n",
      "        than `stop`.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    numpy.linspace : Evenly spaced numbers with careful handling of endpoints.\n",
      "    numpy.ogrid: Arrays of evenly spaced numbers in N-dimensions.\n",
      "    numpy.mgrid: Grid-shaped arrays of evenly spaced numbers in N-dimensions.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> np.arange(3)\n",
      "    array([0, 1, 2])\n",
      "    >>> np.arange(3.0)\n",
      "    array([ 0.,  1.,  2.])\n",
      "    >>> np.arange(3,7)\n",
      "    array([3, 4, 5, 6])\n",
      "    >>> np.arange(3,7,2)\n",
      "    array([3, 5])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(np.arange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00021"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "210e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# A int is getting into the computational graph, it has to be through the sum op\n",
    "class Tensor:\n",
    "    def __init__(self, data, requires_grad=False, _prev=()):\n",
    "        self.data = (data if isinstance(data, np.ndarray) \n",
    "                     else np.array(data)).astype(np.float32)\n",
    "        self.grad = None\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_prev)\n",
    "        self.grad_fn = str()\n",
    "        self.requires_grad = requires_grad\n",
    "\n",
    "    @classmethod\n",
    "    def kaiming_uniform(cls, in_dims, out_dims, **kwargs):\n",
    "        limit = np.sqrt(6 / in_dims)\n",
    "        data = np.random.uniform(-limit, limit, (in_dims, out_dims))\n",
    "        return cls(data, **kwargs)\n",
    "    \n",
    "    @classmethod\n",
    "    def zeros(cls, shape: tuple, **kwargs):\n",
    "        return cls(np.zeros(shape), **kwargs)\n",
    "    \n",
    "    @classmethod\n",
    "    def ones(cls, shape: tuple, **kwargs):\n",
    "        return cls(np.ones(shape), **kwargs)\n",
    "    \n",
    "    @classmethod\n",
    "    def eye(cls, shape: tuple, **kwargs):\n",
    "        return cls(np.eye(shape), **kwargs)\n",
    "    \n",
    "    @classmethod\n",
    "    def randn(cls, shape: tuple, **kwargs):\n",
    "        return cls(np.random.randn(*shape), **kwargs)\n",
    "    \n",
    "    @classmethod\n",
    "    def arange(cls, start, stop, step, **kwargs):\n",
    "        return cls(np.arange(start, stop, step), **kwargs)\n",
    "    \n",
    "    @property\n",
    "    def T(self):\n",
    "        out = Tensor(self.data.T, _prev=(self,))\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                if self.grad is None:\n",
    "                    self.grad = np.zeros_like(self.data)\n",
    "                self.grad += out.grad.T\n",
    "        \n",
    "        if self.requires_grad:\n",
    "            out.requires_grad = True\n",
    "            out._backward = _backward\n",
    "            out.grad_fn = \"TransposeBackward\"\n",
    "    \n",
    "        return out\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.data.shape\n",
    "    \n",
    "    @property\n",
    "    def ndims(self):\n",
    "        return len(self.shape)\n",
    "\n",
    "    def reshape(self, *args):\n",
    "        # Need to implement backward\n",
    "        return Tensor(self.data.reshape(*args), requires_grad=self.requires_grad, _prev=(self,))\n",
    "        \n",
    "    def __matmul__(self, other):\n",
    "        assert self.shape[1] == other.shape[0], f\"{self.shape}, {other.shape}: Incompatible shapes for matrix multiplication.\"\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        out = Tensor(np.dot(self.data, other.data), _prev=(self, other))\n",
    "            \n",
    "        def _backward():\n",
    "            # lazy grad init for efficiancy\n",
    "            if self.requires_grad:\n",
    "                if self.grad is None:\n",
    "                    self.grad = np.zeros_like(self.data)\n",
    "                self.grad += np.dot(out.grad, other.data.T)\n",
    "\n",
    "            if other.requires_grad:\n",
    "                if other.grad is None:\n",
    "                    other.grad = np.zeros_like(other.data)\n",
    "                other.grad += np.dot(self.data.T, out.grad)\n",
    "\n",
    "        if self.requires_grad or other.requires_grad:\n",
    "            out.requires_grad = True\n",
    "            out._backward = _backward\n",
    "            out.grad_fn = \"DotBackward\"\n",
    "            \n",
    "        return out\n",
    "        \n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        out = Tensor(self.data + other.data, _prev=(self, other))\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                if self.grad is None:\n",
    "                    self.grad = np.zeros_like(self.data)\n",
    "                grad_self, _ = np.broadcast_arrays(out.grad, self.data)\n",
    "                axes_to_reduce = tuple(i for i in range(grad_self.ndim) if self.data.shape[i] != grad_self.shape[i])\n",
    "                self.grad += np.sum(grad_self, axis=axes_to_reduce, keepdims=True)\n",
    "\n",
    "            if other.requires_grad:\n",
    "                if other.grad is None:\n",
    "                    other.grad = np.zeros_like(other.data)\n",
    "                _, grad_other = np.broadcast_arrays(out.grad, other.data)\n",
    "                axes_to_reduce = tuple(i for i in range(grad_other.ndim) if other.data.shape[i] != grad_other.shape[i])\n",
    "                other.grad += np.sum(grad_other, axis=axes_to_reduce, keepdims=True)\n",
    "\n",
    "        if self.requires_grad or other.requires_grad:\n",
    "            out.requires_grad = True\n",
    "            out._backward = _backward\n",
    "            out.grad_fn = \"AddBackward\"\n",
    "\n",
    "        return out\n",
    "\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        out = Tensor(self.data * other.data, _prev=(self, other))\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                if self.grad is None:\n",
    "                    self.grad = np.zeros_like(self.data)\n",
    "                self.grad += other.data * out.grad\n",
    "            if other.requires_grad:\n",
    "                if other.grad is None:\n",
    "                    other.grad = np.zeros_like(self.data)\n",
    "                other.grad += self.data * out.grad\n",
    "                \n",
    "        if self.requires_grad or other.requires_grad:\n",
    "            out.requires_grad = True\n",
    "            out._backward = _backward\n",
    "            out.grad_fn = \"MulBackward\"\n",
    "            \n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (float, int))\n",
    "        out = Tensor(np.power(self.data, other), _prev=(self, ))\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                if self.grad is None:\n",
    "                    self.grad = np.zeros_like(self.data)\n",
    "                self.grad += (other * np.power(self.data, (other - 1))) * out.grad\n",
    "            \n",
    "        if self.requires_grad:\n",
    "            out.requires_grad = True\n",
    "            out._backward = _backward \n",
    "            out.grad_fn = \"PowBackward\"\n",
    "        return out\n",
    "\n",
    "    def relu(self):\n",
    "        out = Tensor(np.maximum(0, self.data), _prev=(self,))\n",
    "        \n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                if self.grad is None:\n",
    "                    self.grad = np.zeros_like(self.data)\n",
    "                self.grad += (out.data > 0) * out.grad\n",
    "            \n",
    "        if self.requires_grad:\n",
    "            out.requires_grad = True\n",
    "            out._backward = _backward\n",
    "            out.grad_fn = \"ReluBackward\"\n",
    "        return out\n",
    "\n",
    "    def tanh(self):\n",
    "        n = self.data\n",
    "        t = (np.exp(2*n) - 1)/(np.exp(2*n)+1) # Calc tanh\n",
    "        out = Tensor(t, _prev=(self, ))\n",
    "        \n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                if self.grad is None:\n",
    "                    self.grad = np.zeros_like(self.data)\n",
    "                self.grad += (1 - t**2) * out.grad\n",
    "        \n",
    "        if self.requires_grad:\n",
    "            out.requires_grad = True   \n",
    "            out._backward = _backward\n",
    "            out.grad_fn = \"TanhBackward\"\n",
    "        return out\n",
    "\n",
    "    def sigmoid(self):\n",
    "        x = self.data\n",
    "        s = 1/(1 + np.exp(-x))\n",
    "        out = Tensor(s, _prev=(self, ))\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad: #enables dynamic graph behavior\n",
    "                if self.grad is None:\n",
    "                    self.grad = np.zeros_like(self.data)\n",
    "                self.grad += out.data * (1 - out.data) * out.grad\n",
    "\n",
    "        if self.requires_grad:\n",
    "            out.requires_grad = True   \n",
    "            out._backward = _backward\n",
    "            out.grad_fn = \"SigmoidBackward\"\n",
    "        return out\n",
    "        \n",
    "    def exp(self):\n",
    "        out = Tensor(np.exp(self.data), _prev=(self, ))\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                if self.grad is None:\n",
    "                    self.grad = np.zeros_like(self.data)\n",
    "                self.grad += out.data * out.grad\n",
    "\n",
    "        if self.requires_grad:\n",
    "            out.requires_grad = True \n",
    "            out._backward = _backward\n",
    "            out.grad_fn = \"ExpBackward\"\n",
    "        return out\n",
    "\n",
    "    def log(self):\n",
    "        out = Tensor(np.log(self.data), _prev=(self, ))\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                if self.grad is None:\n",
    "                    self.grad = np.zeros_like(self.data)\n",
    "                self.grad += (1/self.data) * out.grad\n",
    "\n",
    "        if self.requires_grad:\n",
    "            out.requires_grad = True\n",
    "            out._backward = _backward\n",
    "            out.grad_fn = \"LogBackward\"\n",
    "        return out\n",
    "    \n",
    "    def sin(self):\n",
    "        out = Tensor(np.sin(self.data), _prev=(self, ))\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                if self.grad is None:\n",
    "                    self.grad = np.zeros_like(self.data)\n",
    "                self.grad += np.cos(self.data) * out.grad\n",
    "        \n",
    "        if self.requires_grad:\n",
    "            out.requires_grad = True\n",
    "            out._backward = _backward\n",
    "            out.grad_fn = \"SinBackward\"\n",
    "        return out\n",
    "    \n",
    "    # reduce op\n",
    "    def sum(self, axis=None, keepdims=False):\n",
    "        result_data = np.sum(self.data, axis=axis, keepdims=keepdims)\n",
    "        out = Tensor(result_data, _prev=(self,))\n",
    "\n",
    "        def _backward():\n",
    "            if self.requires_grad:\n",
    "                if self.grad is None:\n",
    "                    self.grad = np.zeros_like(self.data)\n",
    "                # is this how the grad should perform?\n",
    "                grad = np.ones_like(self.data)\n",
    "                self.grad += grad * out.grad\n",
    "\n",
    "        if self.requires_grad:\n",
    "            out.requires_grad = True\n",
    "            out._backward = _backward\n",
    "            out.grad_fn = \"SumBackward\"\n",
    "\n",
    "        return out\n",
    "\n",
    "    def mean(self):\n",
    "        pass\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "\n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "        \n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other):\n",
    "        return other + (-self)\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        return self * other**-1\n",
    "\n",
    "    def __rtruediv__(self, other):\n",
    "        return self / other\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.data}\"\n",
    "\n",
    "    def _topological_sort(self):\n",
    "        # Update to khan algorithm\n",
    "        # Review Graph Theory\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "\n",
    "        build_topo(self)\n",
    "        return topo\n",
    "\n",
    "    def _compute_gradients(self, topo_sorted):\n",
    "        self.grad = np.ones_like(self.data)\n",
    "        for node in reversed(topo_sorted):\n",
    "            node._backward()\n",
    "\n",
    "\n",
    "    def backward(self):\n",
    "        topo_sorted = self._topological_sort()\n",
    "        self._compute_gradients(topo_sorted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
