{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "755c5932",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensor import Tensor\n",
    "import numpy as np\n",
    "import nn\n",
    "import torch\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9a3301a-2e4d-4e18-a485-a047128ccfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8216b8a-9f06-406e-bfe6-46f56ce6c716",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8caee9ed-1e86-4f2b-bdb2-29744ca1e6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3465f2fd-ba21-4168-aa51-700886a700dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor())\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1df89e5-fc15-43bf-9b8a-dbead6a22d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=64,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=64,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4cd0f54-9dff-4a59-b294-809954c8ef20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_dataloader:\n",
    "    print(x.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10aa207a-92e1-497d-935d-82acdea23f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3ea560f5-34f8-48ce-a96d-33f61e0da570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(255, dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.data.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd96b13a-822b-4e0c-b531-868af4432307",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97fb7d2d-56e1-4e73-8b9c-1ddbb0f02598",
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_DATA_NUMPY = train_data.data.numpy()\n",
    "MNIST_LABELS_NUMPY = train_data.targets.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17fbea5b-485b-43aa-a46b-bf7e97a596ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82d3b8d0-573f-47af-8f95-f1e878863f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from dataloader import DataLoader\n",
    "\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, data_array, labels):\n",
    "        self.dataset = data_array\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.dataset[index]\n",
    "        labels = self.labels[index]\n",
    "        return Tensor(data), Tensor(labels, dtype=np.int32)\n",
    "\n",
    "nano_data = MNISTDataset(MNIST_DATA_NUMPY, MNIST_LABELS_NUMPY)\n",
    "mnist_loader = DataLoader(nano_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d02437d-235c-476b-bfd3-770824f222df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLPTorch(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_input, num_output):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(num_input, 512)\n",
    "        self.layer_2 = nn.Linear(512, 512)\n",
    "        self.layer_3 = nn.Linear(512, num_output)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x.flatten(1)).relu()\n",
    "        x = self.layer_2(x).relu()\n",
    "        x = self.layer_3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de3e41ef-2eed-4e22-9b03-eee279cc2941",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82f11c70-076f-47eb-8c26-e59540bdb45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPTorch(28*28, 10)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "575d0e84-4e93-4dc3-a88d-e801d361415f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a33f482c-9f3a-4c12-a809-879689ffc753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader.dataset.data.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1bfff5d2-2a4a-420b-8802-c77487306957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4,  ..., 5, 6, 8])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da5fe1e9-6658-4327-a745-e417b188c2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1\n",
      "STEP 0: loss: 18.2470645904541\n",
      "STEP 100: loss: 2.2898802757263184\n",
      "STEP 200: loss: 2.2928245067596436\n",
      "STEP 300: loss: 2.3012189865112305\n",
      "STEP 400: loss: 2.2927191257476807\n",
      "STEP 500: loss: 2.300537109375\n",
      "STEP 600: loss: 2.292175769805908\n",
      "STEP 700: loss: 2.296680212020874\n",
      "STEP 800: loss: 2.311771869659424\n",
      "STEP 900: loss: 2.2965590953826904\n",
      "AVG LOSS:  125653.92261972174\n",
      "EPOCH 2\n",
      "STEP 0: loss: 2.301868200302124\n",
      "STEP 100: loss: 2.297043800354004\n",
      "STEP 200: loss: 2.3195114135742188\n",
      "STEP 300: loss: 2.3101446628570557\n",
      "STEP 400: loss: 2.306162118911743\n",
      "STEP 500: loss: 2.3022921085357666\n",
      "STEP 600: loss: 2.292564630508423\n",
      "STEP 700: loss: 2.306847095489502\n",
      "STEP 800: loss: 2.2947630882263184\n",
      "STEP 900: loss: 2.290712356567383\n",
      "AVG LOSS:  2.3028333712259927\n",
      "EPOCH 3\n",
      "STEP 0: loss: 2.3072876930236816\n",
      "STEP 100: loss: 2.291332244873047\n",
      "STEP 200: loss: 2.3125734329223633\n",
      "STEP 300: loss: 2.298501491546631\n",
      "STEP 400: loss: 2.295417308807373\n",
      "STEP 500: loss: 2.305154800415039\n",
      "STEP 600: loss: 2.2918612957000732\n",
      "STEP 700: loss: 2.291163444519043\n",
      "STEP 800: loss: 2.294044256210327\n",
      "STEP 900: loss: 2.308326482772827\n",
      "AVG LOSS:  2.3028265317281087\n",
      "EPOCH 4\n",
      "STEP 0: loss: 2.3064308166503906\n",
      "STEP 100: loss: 2.289494276046753\n",
      "STEP 200: loss: 2.3035051822662354\n",
      "STEP 300: loss: 2.300934314727783\n",
      "STEP 400: loss: 2.2979683876037598\n",
      "STEP 500: loss: 2.309680938720703\n",
      "STEP 600: loss: 2.3081767559051514\n",
      "STEP 700: loss: 2.2984118461608887\n",
      "STEP 800: loss: 2.3035316467285156\n",
      "STEP 900: loss: 2.2905609607696533\n",
      "AVG LOSS:  2.302716544596354\n",
      "EPOCH 5\n",
      "STEP 0: loss: 2.3002662658691406\n",
      "STEP 100: loss: 2.307715892791748\n",
      "STEP 200: loss: 2.2930686473846436\n",
      "STEP 300: loss: 2.2986016273498535\n",
      "STEP 400: loss: 2.2953662872314453\n",
      "STEP 500: loss: 2.299793004989624\n",
      "STEP 600: loss: 2.3008501529693604\n",
      "STEP 700: loss: 2.3109195232391357\n",
      "STEP 800: loss: 2.319979667663574\n",
      "STEP 900: loss: 2.29046630859375\n",
      "AVG LOSS:  2.3027538917541506\n",
      "EPOCH 6\n",
      "STEP 0: loss: 2.2934679985046387\n",
      "STEP 100: loss: 2.3041419982910156\n",
      "STEP 200: loss: 2.2988691329956055\n",
      "STEP 300: loss: 2.3031089305877686\n",
      "STEP 400: loss: 2.295774459838867\n",
      "STEP 500: loss: 2.2957236766815186\n",
      "STEP 600: loss: 2.3088371753692627\n",
      "STEP 700: loss: 2.3048460483551025\n",
      "STEP 800: loss: 2.2998948097229004\n",
      "STEP 900: loss: 2.3076400756835938\n",
      "AVG LOSS:  2.30273862991333\n",
      "EPOCH 7\n",
      "STEP 0: loss: 2.304922103881836\n",
      "STEP 100: loss: 2.2929816246032715\n",
      "STEP 200: loss: 2.300271511077881\n",
      "STEP 300: loss: 2.288949489593506\n",
      "STEP 400: loss: 2.3011600971221924\n",
      "STEP 500: loss: 2.3007564544677734\n",
      "STEP 600: loss: 2.312281847000122\n",
      "STEP 700: loss: 2.3010284900665283\n",
      "STEP 800: loss: 2.2965354919433594\n",
      "STEP 900: loss: 2.302407741546631\n",
      "AVG LOSS:  2.302762813313802\n",
      "EPOCH 8\n",
      "STEP 0: loss: 2.3092336654663086\n",
      "STEP 100: loss: 2.299257755279541\n",
      "STEP 200: loss: 2.2994871139526367\n",
      "STEP 300: loss: 2.3048884868621826\n",
      "STEP 400: loss: 2.3100147247314453\n",
      "STEP 500: loss: 2.314403533935547\n",
      "STEP 600: loss: 2.2974789142608643\n",
      "STEP 700: loss: 2.2890501022338867\n",
      "STEP 800: loss: 2.291660785675049\n",
      "STEP 900: loss: 2.2901933193206787\n",
      "AVG LOSS:  2.3028170768737795\n",
      "EPOCH 9\n",
      "STEP 0: loss: 2.303148031234741\n",
      "STEP 100: loss: 2.3164827823638916\n",
      "STEP 200: loss: 2.310438394546509\n",
      "STEP 300: loss: 2.3019256591796875\n",
      "STEP 400: loss: 2.2989680767059326\n",
      "STEP 500: loss: 2.303973913192749\n",
      "STEP 600: loss: 2.292295217514038\n",
      "STEP 700: loss: 2.289522171020508\n",
      "STEP 800: loss: 2.3157012462615967\n",
      "STEP 900: loss: 2.312617063522339\n",
      "AVG LOSS:  2.30282139561971\n",
      "EPOCH 10\n",
      "STEP 0: loss: 2.3017325401306152\n",
      "STEP 100: loss: 2.3092401027679443\n",
      "STEP 200: loss: 2.3100223541259766\n",
      "STEP 300: loss: 2.298504114151001\n",
      "STEP 400: loss: 2.3055570125579834\n",
      "STEP 500: loss: 2.291882038116455\n",
      "STEP 600: loss: 2.2843360900878906\n",
      "STEP 700: loss: 2.298139810562134\n",
      "STEP 800: loss: 2.302645206451416\n",
      "STEP 900: loss: 2.3166725635528564\n",
      "AVG LOSS:  2.3027207229614257\n"
     ]
    }
   ],
   "source": [
    "model = MLPTorch(28*28, 10)\n",
    "EPOCHS = 10\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "total_items = train_dataloader.dataset.data.size()[0] / train_dataloader.batch_size\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_train_loss = 0\n",
    "    print(f\"EPOCH {epoch + 1}\")\n",
    "    for i, (x, y) in enumerate(train_dataloader):\n",
    "        x *= 255\n",
    "        optim.zero_grad()\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "        if i % 100 == 0:\n",
    "            print(f\"STEP {i}: loss: {loss.item()}\")\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total_train_loss += loss.item()\n",
    "    print(\"AVG LOSS: \", total_train_loss / total_items)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4700d366-1c05-4d46-bd13-3fbacf0075d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    print(num_batches)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X *= 255\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ccc8b9e5-53d3-456b-a4de-a1e23988eb6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ceil(10000 / 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47daf9fc-7d2d-40b7-9ccd-6df6a787fa90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157\n",
      "Test Error: \n",
      " Accuracy: 10.3%, Avg loss: 2.301301 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loop(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c8cbb56f-ecef-4732-8da1-6d8d7743a97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = torch.randn(10, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6473b2f1-98bf-4394-aa57-fe9dd2359a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 784])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy.flatten(1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cd5e95f1-56a7-4f1e-98c5-5f3fca2ea036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0232,  0.0194, -0.0318, -0.0353, -0.1938, -0.0837, -0.1184, -0.2372,\n",
       "         -0.1115,  0.0053],\n",
       "        [ 0.0622,  0.0311, -0.0318, -0.0962, -0.0816, -0.0509, -0.0692, -0.1975,\n",
       "          0.0434,  0.0465],\n",
       "        [-0.1591,  0.1465, -0.1142, -0.0677, -0.1329, -0.0687,  0.0468, -0.0051,\n",
       "         -0.0197, -0.0347],\n",
       "        [-0.0403,  0.0543, -0.0114, -0.0193, -0.1147, -0.0528, -0.0598, -0.0489,\n",
       "         -0.0483,  0.0926],\n",
       "        [-0.1176, -0.0189, -0.0337, -0.0534, -0.1033, -0.0617, -0.0092, -0.0457,\n",
       "          0.0512,  0.0165],\n",
       "        [-0.0781, -0.0003, -0.0652, -0.0910, -0.1181, -0.1104, -0.0648, -0.1068,\n",
       "         -0.0808, -0.0345],\n",
       "        [-0.0383,  0.1365, -0.0366, -0.1716, -0.0395, -0.1997, -0.2151, -0.0797,\n",
       "         -0.0064,  0.1109],\n",
       "        [ 0.0519, -0.0624, -0.0559, -0.0032, -0.1382, -0.1285,  0.0707, -0.0853,\n",
       "         -0.0356,  0.0968],\n",
       "        [ 0.0958,  0.0004,  0.0463, -0.1408, -0.0930, -0.0834, -0.0446, -0.1864,\n",
       "          0.0991, -0.0430],\n",
       "        [ 0.1359,  0.0870, -0.0409,  0.0231, -0.1341,  0.0754, -0.0295, -0.1643,\n",
       "         -0.0221,  0.0941]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLPTorch(28*28, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d7449e6-ed77-4850-911f-e194bc848f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nn\n",
    "from tensor import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64f4f384-7b6b-4ca5-bc27-3ec6010d547f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nn\n",
    "from tensor import Tensor\n",
    "\n",
    "from datasets import Dataset\n",
    "from dataloader import DataLoader\n",
    "\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, data_array, labels):\n",
    "        self.dataset = data_array\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.dataset[index]\n",
    "        labels = self.labels[index]\n",
    "        return Tensor(data), Tensor(labels, dtype=np.int32)\n",
    "\n",
    "nano_data = MNISTDataset(MNIST_DATA_NUMPY, MNIST_LABELS_NUMPY)\n",
    "mnist_loader = DataLoader(nano_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7fa72760-2381-490c-b6e0-dd437e58df79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNano(nn.Module):\n",
    "    def __init__(self, num_input, num_output):\n",
    "        self.layer_1 = nn.Linear(num_input, 512)\n",
    "        self.layer_2 = nn.Linear(512, 512)\n",
    "        self.layer_3 = nn.Linear(512, num_output)\n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x.flatten(1)).relu()\n",
    "        x = self.layer_2(x).relu()\n",
    "        x = self.layer_3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f37a912-9d1a-4156-b2c5-27f1f954ed79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNIST_DATA_NUMPY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "351f8c7d-daba-4d36-980c-0ad62709df9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "for a, b in mnist_loader:\n",
    "    print(a.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52caef97-14cd-4c38-a7fc-9a06b399ef24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "937.5\n",
      "EPOCH: 1\n",
      "AVG LOSS: 0.49842444287935894\n",
      "\n",
      "\n",
      "EPOCH: 2\n",
      "AVG LOSS: 0.33594805908203124\n",
      "\n",
      "\n",
      "EPOCH: 3\n",
      "AVG LOSS: 0.3010612559914589\n",
      "\n",
      "\n",
      "EPOCH: 4\n",
      "AVG LOSS: 0.0908853342935443\n",
      "\n",
      "\n",
      "EPOCH: 5\n",
      "AVG LOSS: 0.045134065474073094\n",
      "\n",
      "\n",
      "EPOCH: 6\n",
      "AVG LOSS: 0.03298274559279283\n",
      "\n",
      "\n",
      "EPOCH: 7\n",
      "AVG LOSS: 0.025991431546459595\n",
      "\n",
      "\n",
      "EPOCH: 8\n",
      "AVG LOSS: 0.01878465625386064\n",
      "\n",
      "\n",
      "EPOCH: 9\n",
      "AVG LOSS: 0.01475584541761006\n",
      "\n",
      "\n",
      "EPOCH: 10\n",
      "AVG LOSS: 0.011574254630164553\n",
      "\n",
      "\n",
      "TIME: 54.43138313293457\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "train_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor())\n",
    "train_dataloader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=64,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# def test_loop(dataloader, model, loss_fn):\n",
    "#     # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "#     size = len(dataloader.dataset)\n",
    "#     num_batches = len(dataloader)\n",
    "#     test_loss, correct = 0, 0\n",
    "\n",
    "#     # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "#     # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "#     with torch.no_grad():\n",
    "#         for X, y in dataloader:\n",
    "#             pred = model(X)\n",
    "#             test_loss += loss_fn(pred, y).item()\n",
    "#             correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "#     test_loss /= num_batches\n",
    "#     correct /= size\n",
    "#     print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "\n",
    "MNIST_DATA_NUMPY = train_data.data.numpy() / 255\n",
    "MNIST_LABELS_NUMPY = train_data.targets.numpy()\n",
    "\n",
    "\n",
    "import nn\n",
    "from tensor import Tensor\n",
    "\n",
    "from datasets import Dataset\n",
    "from dataloader import DataLoader\n",
    "\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, data_array, labels):\n",
    "        self.dataset = data_array\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.dataset[index]\n",
    "        labels = self.labels[index]\n",
    "        return Tensor(data), Tensor(labels, dtype=np.int32)\n",
    "\n",
    "nano_data = MNISTDataset(MNIST_DATA_NUMPY, MNIST_LABELS_NUMPY)\n",
    "mnist_loader = DataLoader(nano_data, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "class MLPNano(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_input, num_output):\n",
    "        self.layer_1 = nn.Linear(num_input, 512)\n",
    "        self.layer_2 = nn.Linear(512, 512)\n",
    "        self.layer_3 = nn.Linear(512, num_output)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x.flatten(1)).relu()\n",
    "        x = self.layer_2(x).relu()\n",
    "        x = self.layer_3(x).relu()\n",
    "        return x\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "dummy = Tensor.randn((10, 1, 28, 28))\n",
    "model = MLPNano(28*28, 10)\n",
    "optim = nn.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "start_time = time.time() \n",
    "total_items = train_dataloader.dataset.data.size()[0] / train_dataloader.batch_size\n",
    "print(total_items)\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"EPOCH: {epoch+1}\")\n",
    "    total_train_loss = 0\n",
    "    for i , (x, y) in enumerate(mnist_loader):\n",
    "        optim.zero_grad()\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total_train_loss += loss.item()\n",
    "    print(\"AVG LOSS:\", total_train_loss / total_items)\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "end_time = time.time()  # Record the end time\n",
    "elapsed_time = end_time - start_time  # Calculate the elapsed time   \n",
    "\n",
    "print(\"TIME:\", elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46cd6637-856e-4d57-bf1d-9b2a7bae0572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 99.9%, Avg loss: 0.007821 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dataloader import DataLoader\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "MNIST_DATA_NUMPY = test_data.data.numpy() / 255\n",
    "MNIST_LABELS_NUMPY = test_data.targets.numpy()\n",
    "\n",
    "test_data = MNISTDataset(MNIST_DATA_NUMPY, MNIST_LABELS_NUMPY)\n",
    "test_loader = DataLoader(nano_data, batch_size=64, shuffle=True)\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = 60000 / 64 #len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    for X, y in dataloader:\n",
    "        pred = model(X)\n",
    "        test_loss += loss_fn(pred, y).item()\n",
    "        correct += (pred.argmax(1) == y).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "\n",
    "test_loop(test_loader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad5c8eb7-f2b3-4139-9c21-0ae8f1b7d81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.fc1 = Linear(28 * 28, 256)\n",
    "        self.fc2 = Linear(256, 256)  # Second hidden layer, 256 neurons\n",
    "        self.output = Linear(256, 10)  # Output layer, 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x).relu()\n",
    "        x = self.fc2(x).relu()\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d438abec-1b6e-4af0-838f-9458f946e991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-images-idx3-ubyte.gz...\n",
      "Downloaded train-images-idx3-ubyte.gz\n",
      "Downloading train-labels-idx1-ubyte.gz...\n",
      "Downloaded train-labels-idx1-ubyte.gz\n",
      "Downloading t10k-images-idx3-ubyte.gz...\n",
      "Downloaded t10k-images-idx3-ubyte.gz\n",
      "Downloading t10k-labels-idx1-ubyte.gz...\n",
      "Downloaded t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "ename": "BadGzipFile",
     "evalue": "Not a gzipped file (b'<!')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadGzipFile\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize dataset and dataloader\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMNIST\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmnist_data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mMNIST(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmnist_data\u001b[39m\u001b[38;5;124m'\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Developer/dl_work/Learning/Explore/NanoGrad/datasets.py:30\u001b[0m, in \u001b[0;36mMNIST.__init__\u001b[0;34m(self, root, train, transform)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_images\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_labels(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Developer/dl_work/Learning/Explore/NanoGrad/datasets.py:61\u001b[0m, in \u001b[0;36mMNIST.load_images\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_images\u001b[39m(file_path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m gzip\u001b[38;5;241m.\u001b[39mopen(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 61\u001b[0m         data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, np\u001b[38;5;241m.\u001b[39muint8, offset\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m28\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ptb/lib/python3.10/gzip.py:301\u001b[0m, in \u001b[0;36mGzipFile.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01merrno\u001b[39;00m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(errno\u001b[38;5;241m.\u001b[39mEBADF, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread() on write-only GzipFile object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ptb/lib/python3.10/_compression.py:118\u001b[0m, in \u001b[0;36mDecompressReader.readall\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# sys.maxsize means the max length of output buffer is unlimited,\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# so that the whole input buffer can be decompressed within one\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# .decompress() call.\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m data \u001b[38;5;241m:=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaxsize\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    119\u001b[0m     chunks\u001b[38;5;241m.\u001b[39mappend(data)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(chunks)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ptb/lib/python3.10/gzip.py:488\u001b[0m, in \u001b[0;36m_GzipReader.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_member:\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;66;03m# If the _new_member flag is set, we have to\u001b[39;00m\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;66;03m# jump to the next member, if there is one.\u001b[39;00m\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_read()\n\u001b[0;32m--> 488\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_gzip_header\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    489\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pos\n\u001b[1;32m    490\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ptb/lib/python3.10/gzip.py:436\u001b[0m, in \u001b[0;36m_GzipReader._read_gzip_header\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\037\u001b[39;00m\u001b[38;5;130;01m\\213\u001b[39;00m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 436\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadGzipFile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNot a gzipped file (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m magic)\n\u001b[1;32m    438\u001b[0m (method, flag,\n\u001b[1;32m    439\u001b[0m  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_mtime) \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<BBIxx\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_exact(\u001b[38;5;241m8\u001b[39m))\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m8\u001b[39m:\n",
      "\u001b[0;31mBadGzipFile\u001b[0m: Not a gzipped file (b'<!')"
     ]
    }
   ],
   "source": [
    "# Initialize dataset and dataloader\n",
    "train_dataset = datasets.MNIST(root='mnist_data', train=True)\n",
    "test_dataset = datasets.MNIST(root='mnist_data', train=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c6d3897",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MLP.__init__() missing 2 required positional arguments: 'n_in' and 'n_out'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m\n\u001b[1;32m      3\u001b[0m xs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      4\u001b[0m     [\u001b[38;5;241m2.0\u001b[39m, \u001b[38;5;241m3.0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m],\n\u001b[1;32m      5\u001b[0m     [\u001b[38;5;241m3.0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m0.5\u001b[39m],\n\u001b[1;32m      6\u001b[0m     [\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m],\n\u001b[1;32m      7\u001b[0m     [\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m],\n\u001b[1;32m      8\u001b[0m ]\n\u001b[1;32m     10\u001b[0m ys \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m1.0\u001b[39m], \n\u001b[1;32m     11\u001b[0m       [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m], \n\u001b[1;32m     12\u001b[0m       [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m], \n\u001b[1;32m     13\u001b[0m       [\u001b[38;5;241m1.0\u001b[39m]]\n\u001b[0;32m---> 17\u001b[0m net \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMLP\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m optim \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSGD(net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.005\u001b[39m, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n\u001b[1;32m     21\u001b[0m x \u001b[38;5;241m=\u001b[39m Tensor(xs)\n",
      "\u001b[0;31mTypeError\u001b[0m: MLP.__init__() missing 2 required positional arguments: 'n_in' and 'n_out'"
     ]
    }
   ],
   "source": [
    "## Does it work in simple contexts?\n",
    "\n",
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0],\n",
    "]\n",
    "\n",
    "ys = [[1.0], \n",
    "      [-1.0], \n",
    "      [-1.0], \n",
    "      [1.0]]\n",
    "\n",
    "\n",
    "\n",
    "net = nn.MLP()\n",
    "\n",
    "optim = nn.SGD(net.parameters(), lr=0.005, momentum=0.9)\n",
    "\n",
    "x = Tensor(xs)\n",
    "y_real = Tensor(ys)\n",
    "\n",
    "for k in range(10000):\n",
    "    optim.zero_grad()\n",
    "    y_pred = net(x)\n",
    "    loss = ((y_pred - y_real)**2).mean(0)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if k % 200 == 0:\n",
    "        print(k, loss.data)\n",
    "    \n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0ae1129-ddb7-4772-91f6-ed446a8f1b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_circles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0678922d-6bb9-4a77-aeae-754dc3b527dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2acb07c0-72b2-429f-b89b-6eabaaeb9386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.37988947,  0.05099247],\n",
       "        [ 0.58493451, -0.798081  ],\n",
       "        [ 1.04167054,  0.74203373],\n",
       "        [-2.86375784,  0.58412593],\n",
       "        [ 1.03509501,  0.47722211],\n",
       "        [ 0.537244  ,  1.18953602],\n",
       "        [ 0.59130794, -0.73619249],\n",
       "        [ 0.34538168, -0.0382666 ],\n",
       "        [-1.11921492,  1.06750063],\n",
       "        [ 0.88444262,  1.47938184],\n",
       "        [-0.02414129,  0.74867554],\n",
       "        [-0.69657555, -0.86273344],\n",
       "        [-2.35665649,  0.42522904],\n",
       "        [ 1.300929  , -2.67400544],\n",
       "        [-2.37987777,  0.01012245],\n",
       "        [-0.82709791, -1.7800778 ],\n",
       "        [-1.37558475, -0.40835085],\n",
       "        [-1.09512129, -1.5971113 ],\n",
       "        [-1.50145152, -0.8248431 ],\n",
       "        [ 0.12458102, -1.38277224],\n",
       "        [ 0.8473867 ,  0.84127458],\n",
       "        [ 0.69786211, -0.50552855],\n",
       "        [ 0.58246639, -0.2890588 ],\n",
       "        [ 0.79505177,  0.32554748],\n",
       "        [ 0.56900632,  0.2725109 ],\n",
       "        [ 1.33618627,  0.30505305],\n",
       "        [ 0.09898389, -0.18741819],\n",
       "        [-0.11827806,  0.00755403],\n",
       "        [-0.69351011,  0.33028676],\n",
       "        [ 1.77573364, -0.40765328],\n",
       "        [-1.0107766 ,  0.42055541],\n",
       "        [-2.26436058,  0.19131994],\n",
       "        [ 0.43816138, -0.27469023],\n",
       "        [-1.41899348, -0.48235612],\n",
       "        [-1.18546504,  1.02442576],\n",
       "        [-0.52782265,  0.3507548 ],\n",
       "        [ 0.41158961,  0.79155638],\n",
       "        [ 1.10045588, -1.45553977],\n",
       "        [-0.80413664, -0.20968982],\n",
       "        [ 1.23318374, -0.01839279],\n",
       "        [ 0.69798403,  2.92893306],\n",
       "        [-1.82817587,  0.32898068],\n",
       "        [-0.63999511, -0.20921829],\n",
       "        [ 0.02471879, -0.33206057],\n",
       "        [ 1.33340711,  2.21539149],\n",
       "        [-0.93672244, -0.97622056],\n",
       "        [-1.94752412, -0.87524257],\n",
       "        [ 0.46518867,  0.28500934],\n",
       "        [-0.55070157,  1.89056753],\n",
       "        [-1.67278848,  0.08405816],\n",
       "        [ 0.47077916, -0.50424129],\n",
       "        [ 0.44108986, -0.51231659],\n",
       "        [ 0.13327889,  0.43750864],\n",
       "        [ 2.17962917,  1.06195707],\n",
       "        [ 1.56639207, -0.55027888],\n",
       "        [ 1.2868969 , -0.0747265 ],\n",
       "        [-0.16335375, -0.76123713],\n",
       "        [-0.58380922, -1.07197534],\n",
       "        [ 0.24182534, -0.23737752],\n",
       "        [ 1.32730085, -1.19414438],\n",
       "        [ 1.15635133,  1.37507751],\n",
       "        [-0.37915633,  2.29970715],\n",
       "        [ 1.43041319,  0.45168681],\n",
       "        [ 0.41946162, -1.08164207],\n",
       "        [-1.42719372,  1.0493021 ],\n",
       "        [ 0.85226279, -0.49321787],\n",
       "        [ 0.59417731, -0.906138  ],\n",
       "        [ 0.66377907,  0.69394669],\n",
       "        [ 0.19398645, -0.38439749],\n",
       "        [-0.80773513,  1.1135406 ],\n",
       "        [-0.25153607, -0.44026887],\n",
       "        [-0.45026878, -0.04220967],\n",
       "        [-0.45480343,  0.61875355],\n",
       "        [-0.90940946,  0.5304045 ],\n",
       "        [ 0.64668822, -0.63753276],\n",
       "        [ 0.75338501,  1.66391678],\n",
       "        [-0.47631056, -1.17759944],\n",
       "        [-0.33075714,  0.18044857],\n",
       "        [ 1.19598076,  0.48196925],\n",
       "        [ 1.54488699,  0.36604384],\n",
       "        [-0.57718664, -0.98372169],\n",
       "        [-0.22595872, -0.8160223 ],\n",
       "        [-0.36413556,  1.3458003 ],\n",
       "        [-0.1997047 ,  0.90182089],\n",
       "        [ 0.84347315,  1.48294798],\n",
       "        [-1.53920655, -0.41509806],\n",
       "        [-0.56378133, -0.78313254],\n",
       "        [-0.34311593, -0.30820757],\n",
       "        [ 0.00833126,  0.22065134],\n",
       "        [-2.20106925, -0.84308586],\n",
       "        [ 0.20544721,  0.03514831],\n",
       "        [-2.0902381 ,  0.86229163],\n",
       "        [-1.26505368, -0.04928587],\n",
       "        [ 0.83358281,  0.01701371],\n",
       "        [-0.31737161,  1.45986149],\n",
       "        [-0.85110062,  0.29666534],\n",
       "        [-1.52528517, -0.48057733],\n",
       "        [ 0.47884904,  0.70962502],\n",
       "        [ 1.36764991,  1.35681389],\n",
       "        [ 2.16748119, -1.29302935],\n",
       "        [ 3.00173762, -0.11528646],\n",
       "        [ 0.95530399,  0.06449974],\n",
       "        [-0.64915234,  0.5573208 ],\n",
       "        [-0.61128921,  1.22891441],\n",
       "        [ 1.22817447, -0.02701282],\n",
       "        [ 0.68569501,  0.15170628],\n",
       "        [ 1.67862653,  1.008975  ],\n",
       "        [ 2.35443112, -1.22858669],\n",
       "        [ 1.31867825, -1.28526682],\n",
       "        [-0.08553136, -0.19556585],\n",
       "        [-0.96657389,  0.34773522],\n",
       "        [ 0.11865322, -0.36513261],\n",
       "        [ 1.55313703,  1.79171436],\n",
       "        [ 1.5310555 , -1.00208088],\n",
       "        [ 1.42195234, -1.03836684],\n",
       "        [ 0.57989614, -1.55812776],\n",
       "        [ 0.6579925 ,  0.54703247],\n",
       "        [ 0.58772886,  0.22780292],\n",
       "        [ 0.69625082,  2.19803572],\n",
       "        [ 1.06992838, -0.48019353],\n",
       "        [-0.01983245, -0.71844576],\n",
       "        [ 1.44012566, -0.05377079],\n",
       "        [-1.5194356 ,  1.22713173],\n",
       "        [-0.45064541,  1.94948949],\n",
       "        [ 0.99438865, -0.45594984],\n",
       "        [-1.04674445,  0.33124858],\n",
       "        [-0.5419757 , -0.75876409],\n",
       "        [-1.0109288 , -0.41790997],\n",
       "        [-1.25394269,  0.7900872 ],\n",
       "        [ 1.36817788,  1.31561941],\n",
       "        [ 0.96659834, -0.84114831],\n",
       "        [ 0.41219768, -0.59502253],\n",
       "        [ 1.00472818, -0.3739419 ],\n",
       "        [ 2.35758634, -0.33745144],\n",
       "        [-1.1165868 , -0.10350087],\n",
       "        [ 0.67927194, -0.1631168 ],\n",
       "        [-2.09199714,  1.4852254 ],\n",
       "        [ 0.09496386, -0.76739172],\n",
       "        [-0.59142567,  0.40984373],\n",
       "        [ 1.39148141, -0.10861629],\n",
       "        [-1.32573581, -0.91850265],\n",
       "        [ 0.97792164, -0.92721766],\n",
       "        [-1.74939945,  1.79651434],\n",
       "        [-0.05134676,  0.98935933],\n",
       "        [-0.89082077, -0.67131758],\n",
       "        [ 0.17894723, -0.26370488],\n",
       "        [ 0.37850155, -1.930884  ],\n",
       "        [ 0.51102147, -0.48640013],\n",
       "        [-1.82732894, -0.08511263],\n",
       "        [ 0.78662902,  1.5951093 ],\n",
       "        [ 0.06180105, -1.15569617],\n",
       "        [ 1.72674568, -0.86122886],\n",
       "        [ 0.42685309,  0.67017831],\n",
       "        [ 0.62230728,  1.45711153],\n",
       "        [-0.77627345, -0.59952327],\n",
       "        [ 0.07673571,  0.38990407],\n",
       "        [-0.83312568, -0.44702268],\n",
       "        [-0.29992654, -0.07709146],\n",
       "        [ 0.99589033,  0.8459378 ],\n",
       "        [ 0.35629783,  1.25680619],\n",
       "        [-1.28375134, -0.67590075],\n",
       "        [ 0.44141989, -1.16934015],\n",
       "        [ 1.38271745,  1.08303   ],\n",
       "        [ 0.34674087, -1.19718848],\n",
       "        [-0.91765546, -0.6389312 ],\n",
       "        [-1.1883507 ,  0.74220057],\n",
       "        [ 0.3559945 , -1.35655821],\n",
       "        [-0.14158211,  0.64620796],\n",
       "        [ 0.4737616 ,  1.05037768],\n",
       "        [ 0.13172649, -1.51578997],\n",
       "        [-0.61823653,  1.42693682],\n",
       "        [-0.36249774,  1.19361582],\n",
       "        [ 0.64423934,  0.3780433 ],\n",
       "        [-0.04336982, -0.07898442],\n",
       "        [ 0.3809852 , -0.04315838],\n",
       "        [ 2.74464138, -0.87814266],\n",
       "        [-0.43049792,  1.08824768],\n",
       "        [ 0.07774692,  1.58455932],\n",
       "        [-0.92961526,  1.0124561 ],\n",
       "        [-0.45669479, -0.76872361],\n",
       "        [ 1.17824879, -2.29445671],\n",
       "        [ 0.14796376, -1.63401958],\n",
       "        [ 0.22657891,  0.91956943],\n",
       "        [ 1.70348375,  2.52416318],\n",
       "        [-0.32560588,  0.93869177],\n",
       "        [ 1.06405603, -0.33603559],\n",
       "        [ 0.20476318, -2.38062816],\n",
       "        [-1.01336885, -0.54470955],\n",
       "        [-1.33407744,  1.57792152],\n",
       "        [-2.10297772,  0.43945797],\n",
       "        [-0.16716414, -0.30856934],\n",
       "        [-1.90088733,  0.98908319],\n",
       "        [-0.10712352,  1.90005839],\n",
       "        [ 0.44656093,  0.57017234],\n",
       "        [ 1.00416151,  0.44175218],\n",
       "        [ 0.9277659 ,  0.37449185],\n",
       "        [-0.56066591,  0.413027  ],\n",
       "        [ 0.21565258,  1.45262151],\n",
       "        [-0.08999307, -0.34774259],\n",
       "        [ 1.29126573,  0.91664125]]),\n",
       " array([-1.65359040e+01, -3.89629325e+01,  1.24283532e+02, -1.06769760e+02,\n",
       "         1.00215919e+02,  1.36329000e+02, -3.30578881e+01,  1.57576790e+01,\n",
       "         3.34123196e+01,  1.81563991e+02,  6.56656010e+01, -1.15910818e+02,\n",
       "        -9.28096766e+01, -1.67117859e+02, -1.31265610e+02, -2.05264290e+02,\n",
       "        -1.12957223e+02, -2.03802998e+02, -1.57229111e+02, -1.16859702e+02,\n",
       "         1.22390122e+02, -6.49895691e+00,  6.47766657e+00,  7.32875417e+01,\n",
       "         5.60005778e+01,  1.01505488e+02, -1.12787053e+01, -5.88986089e+00,\n",
       "        -8.95528294e+00,  6.21450804e+01, -1.84747329e+01, -1.08625549e+02,\n",
       "        -2.41048801e-01, -1.21994119e+02,  2.58758428e+01,  2.08621300e+00,\n",
       "         9.37170073e+01, -6.91870000e+01, -6.34212647e+01,  6.68560542e+01,\n",
       "         3.00979929e+02, -7.20946238e+01, -5.42790357e+01, -2.83589348e+01,\n",
       "         2.72363090e+02, -1.39421612e+02, -1.86513814e+02,  5.13367576e+01,\n",
       "         1.38667386e+02, -8.53747562e+01, -1.90026912e+01, -2.13608778e+01,\n",
       "         4.65714538e+01,  2.16111845e+02,  3.77310987e+01,  6.47839994e+01,\n",
       "        -7.72204521e+01, -1.28388408e+02, -7.82406480e+00, -3.31835351e+01,\n",
       "         1.87337253e+02,  1.84815002e+02,  1.19889300e+02, -7.35337508e+01,\n",
       "         1.46756927e+01,  3.15917841e+00, -4.81134008e+01,  9.89929506e+01,\n",
       "        -2.36564443e+01,  5.48243705e+01, -5.33780728e+01, -2.87950421e+01,\n",
       "         3.01263092e+01, -3.02371662e+00, -2.11532826e+01,  1.90810404e+02,\n",
       "        -1.31883292e+02, -2.22327232e+00,  1.09547862e+02,  1.18553254e+02,\n",
       "        -1.20118950e+02, -8.56018095e+01,  1.00244478e+02,  6.96569150e+01,\n",
       "         1.79593386e+02, -1.22668757e+02, -1.01423692e+02, -4.66434495e+01,\n",
       "         2.02027741e+01, -1.97720824e+02,  1.45637644e+01, -3.88947274e+01,\n",
       "        -7.46715153e+01,  4.78070954e+01,  1.13058148e+02, -2.07093957e+01,\n",
       "        -1.27719180e+02,  9.01343942e+01,  1.97405899e+02,  4.62336820e+00,\n",
       "         1.56395316e+02,  5.88388063e+01,  1.38442635e+01,  7.60652969e+01,\n",
       "         6.57864477e+01,  5.16484740e+01,  1.83554573e+02,  2.07752384e+01,\n",
       "        -4.18173901e+01, -2.22570890e+01, -2.25458486e+01, -2.61044828e+01,\n",
       "         2.46664515e+02, -4.67269090e+00, -1.39713026e+01, -1.07276082e+02,\n",
       "         8.55305005e+01,  5.30336542e+01,  2.35427123e+02,  1.64296194e+01,\n",
       "        -6.54110754e+01,  7.51855942e+01,  2.54713303e+01,  1.49487632e+02,\n",
       "         1.44234668e+01, -2.85080845e+01, -9.80395801e+01, -9.35483385e+01,\n",
       "         1.08561341e+00,  1.93772035e+02, -2.16097755e+01, -3.03736469e+01,\n",
       "         2.23179451e+01,  1.00709885e+02, -7.12882647e+01,  2.31261760e+01,\n",
       "         1.67543494e+01, -6.34136634e+01,  3.84291371e+00,  6.75576692e+01,\n",
       "        -1.55874415e+02, -2.86855552e+01,  6.36758797e+01,  8.57132642e+01,\n",
       "        -1.09585252e+02, -1.36756820e+01, -1.51798744e+02, -1.51711965e+01,\n",
       "        -1.09112580e+02,  1.86475756e+02, -1.00023679e+02,  1.88035516e+01,\n",
       "         8.36973482e+01,  1.65010167e+02, -9.67801402e+01,  3.91838774e+01,\n",
       "        -8.62814076e+01, -2.35570128e+01,  1.31045293e+02,  1.32297660e+02,\n",
       "        -1.31788457e+02, -8.01530487e+01,  1.73755202e+02, -8.79178715e+01,\n",
       "        -1.08162707e+02,  4.52917816e-01, -1.01669278e+02,  4.99603869e+01,\n",
       "         1.20349538e+02, -1.28383708e+02,  9.34092227e+01,  8.67146921e+01,\n",
       "         6.96287487e+01, -9.48122099e+00,  1.73047841e+01,  7.38292966e+01,\n",
       "         7.35290049e+01,  1.46176736e+02,  3.90077818e+01, -9.41678257e+01,\n",
       "        -1.39975782e+02, -1.38067163e+02,  9.49031913e+01,  3.20586477e+02,\n",
       "         6.59479787e+01,  2.90097665e+01, -2.01738290e+02, -1.05026459e+02,\n",
       "         6.71561253e+01, -7.74619717e+01, -3.69031986e+01, -1.70359009e+01,\n",
       "         1.64144292e+02,  7.58372267e+01,  9.53181430e+01,  8.50417247e+01,\n",
       "         5.83151740e+00,  1.42021348e+02, -3.61215921e+01,  1.53756286e+02]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.make_regression(n_samples=200, n_features=2, noise=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70d0b0ff-84b1-4e00-889f-18ea95687b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'Tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 39\u001b[0m\n\u001b[1;32m     35\u001b[0m xs \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     36\u001b[0m ys \u001b[38;5;241m=\u001b[39m [[y] \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m Y\u001b[38;5;241m.\u001b[39mtolist()]\n\u001b[0;32m---> 39\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mTensor\u001b[49m(xs)\n\u001b[1;32m     40\u001b[0m y_real \u001b[38;5;241m=\u001b[39m Tensor(ys)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Tensor' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "def generate_twisted_spiral(n_samples, noise=0.05, factor=0.5):\n",
    "    \"\"\"\n",
    "    Generates a spiral dataset using the make_circles function from scikit-learn,\n",
    "    then 'twists' it to resemble a spiral.\n",
    "    :param n_samples: int, total number of points\n",
    "    :param noise: float, standard deviation of Gaussian noise\n",
    "    :param factor: float, scale factor between inner and outer circle\n",
    "    :return: tuple, features (X) and labels (y)\n",
    "    \"\"\"\n",
    "    X, y = make_circles(n_samples=n_samples, factor=factor, noise=noise, random_state=42)\n",
    "    Y = y * 2 - 1  # Adjust labels to be -1 and 1\n",
    "\n",
    "    # Apply a transformation to twist the circles into spirals\n",
    "    transformation = np.array([[0.6, -0.6], [0.8, 0.8]])\n",
    "    X = np.dot(X, transformation)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "# Generate the data\n",
    "n_samples = 200\n",
    "X, Y = generate_twisted_spiral(n_samples)\n",
    "\n",
    "# Plotting the dataset\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.bwr)\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"Twisted Spiral Dataset\")\n",
    "plt.show()\n",
    "\n",
    "# Convert the data for your neural network\n",
    "xs = X.tolist()\n",
    "ys = [[y] for y in Y.tolist()]\n",
    "\n",
    "\n",
    "x = Tensor(xs)\n",
    "y_real = Tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d097b395-abe2-4b6f-b658-cbcf6a8cfbd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9de4d135-e476-4662-9275-cf9a3c75df7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaiming_uniform(shape, gain=np.sqrt(2)):\n",
    "    \"\"\"\n",
    "    Kaiming Uniform initialization (He initialization) for weight matrices.\n",
    "\n",
    "    Args:\n",
    "    shape (tuple): The shape of the weight matrix.\n",
    "    gain (float): The scaling factor (recommended sqrt(2) for ReLU).\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The initialized weights.\n",
    "    \"\"\"\n",
    "    print(shape[:-1])\n",
    "    fan_in = np.prod(shape[:-1])  # Product of dimensions except the last dimension\n",
    "    std = gain / np.sqrt(fan_in)  # Calculate standard deviation\n",
    "    limit = np.sqrt(3.0) * std  # Calculate limit for uniform distribution\n",
    "\n",
    "    return np.random.uniform(-limit, limit, size=shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e14c1c49-2fcc-44a7-8de7-db2a881572e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.3609795497380417"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaiming_uniform((1, 30)).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "831fac3e-bc9e-4dde-a6d2-641aa44e4ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5480296227365216"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div = 1. / np.sqrt(1)\n",
    "\n",
    "np.random.uniform(-div, div, (30, 1)).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f7840a5-b50b-4377-99b7-35c0f3ac7e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3837)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def kaiming_uniform_np(in_features, out_features):\n",
    "    tensor = torch.empty(out_features, in_features)\n",
    "    torch.nn.init.kaiming_uniform_(tensor, a=0)\n",
    "    return tensor\n",
    "\n",
    "kaiming_uniform_np(1, 30).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e08446f-6c8d-428e-8284-d01f1f0222d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0763154"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tensor.kaiming_uniform(1, 30).data.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "32310904-4b96-421a-8ec8-9d6cdc7e6813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6320)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.Linear(1, 30).weight.data.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a225ef2-865d-4d2d-aed8-e59e0a7b1d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbc7557d-0438-462c-8059-332422507072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.34808479e+01],\n",
       "       [ 1.95015536e+01],\n",
       "       [ 9.26908849e+00],\n",
       "       [-7.11581725e+01],\n",
       "       [ 3.82720009e+01],\n",
       "       [-2.02885994e+01],\n",
       "       [-4.51040988e+01],\n",
       "       [-2.99703446e+00],\n",
       "       [-5.36353356e+01],\n",
       "       [ 2.99604916e+01],\n",
       "       [ 2.50319926e+01],\n",
       "       [ 1.04251160e+01],\n",
       "       [ 2.14645088e+01],\n",
       "       [-1.32464234e+01],\n",
       "       [ 6.29139272e+01],\n",
       "       [ 9.49373062e+01],\n",
       "       [ 2.41986366e+00],\n",
       "       [ 2.49414707e+01],\n",
       "       [ 5.90874473e+01],\n",
       "       [-1.32988455e+01],\n",
       "       [-4.08545268e+01],\n",
       "       [ 5.01039319e+01],\n",
       "       [ 9.43388459e+01],\n",
       "       [ 1.31495079e+01],\n",
       "       [ 2.87668639e+01],\n",
       "       [-8.99283457e+01],\n",
       "       [ 5.16602512e+01],\n",
       "       [-8.32871601e+01],\n",
       "       [ 2.80392787e+01],\n",
       "       [-5.47252391e+01],\n",
       "       [ 4.75007538e+00],\n",
       "       [ 1.42287816e+01],\n",
       "       [-3.92285690e+01],\n",
       "       [ 3.04828626e+01],\n",
       "       [-4.81688788e+01],\n",
       "       [ 2.85847082e+00],\n",
       "       [ 1.83038978e+01],\n",
       "       [ 4.37668251e+01],\n",
       "       [-1.28567963e+02],\n",
       "       [-5.74547790e+01],\n",
       "       [ 6.53947587e+01],\n",
       "       [-6.21549007e+00],\n",
       "       [ 3.85965636e+00],\n",
       "       [ 4.98299016e+01],\n",
       "       [ 6.68209875e+01],\n",
       "       [-2.52869079e+01],\n",
       "       [-6.09593921e+01],\n",
       "       [-9.08948338e+00],\n",
       "       [ 6.13655668e+01],\n",
       "       [ 7.23340318e+01],\n",
       "       [-2.69006553e+01],\n",
       "       [-1.53916359e+01],\n",
       "       [-9.35509137e+01],\n",
       "       [ 6.09753113e+00],\n",
       "       [-1.61156747e+01],\n",
       "       [-6.46030781e+01],\n",
       "       [ 1.03491882e+02],\n",
       "       [-6.00392078e+01],\n",
       "       [ 2.61929150e+00],\n",
       "       [-4.76265722e+00],\n",
       "       [ 2.50046075e+01],\n",
       "       [ 1.22860876e+02],\n",
       "       [ 4.11564525e+01],\n",
       "       [ 9.28494602e+00],\n",
       "       [-8.82339018e+01],\n",
       "       [-8.41596177e+00],\n",
       "       [-6.78158181e+01],\n",
       "       [ 3.53466192e+01],\n",
       "       [ 1.28535234e+00],\n",
       "       [ 4.35755617e+01],\n",
       "       [ 7.25589208e+00],\n",
       "       [ 7.57642704e+01],\n",
       "       [ 4.48006481e+00],\n",
       "       [-2.67395776e+01],\n",
       "       [-3.31142780e+00],\n",
       "       [-1.33849616e+02],\n",
       "       [ 8.47523584e+01],\n",
       "       [-1.35231032e+02],\n",
       "       [-1.11664660e+01],\n",
       "       [-5.80380383e+01],\n",
       "       [ 1.05371124e+01],\n",
       "       [-2.42766989e+01],\n",
       "       [ 6.12741505e+01],\n",
       "       [ 8.62848942e+00],\n",
       "       [-1.01627872e+01],\n",
       "       [-7.99076921e+01],\n",
       "       [ 6.04944734e+00],\n",
       "       [-2.83788696e+00],\n",
       "       [-9.30822684e+01],\n",
       "       [ 1.10613781e+01],\n",
       "       [-1.54946140e+01],\n",
       "       [-8.18379894e+01],\n",
       "       [ 1.29112595e+01],\n",
       "       [-5.93188057e+00],\n",
       "       [ 2.25911596e+01],\n",
       "       [ 5.36493557e+01],\n",
       "       [ 8.05646629e+01],\n",
       "       [ 3.07757150e+01],\n",
       "       [ 5.95164759e+01],\n",
       "       [ 7.66166369e+01],\n",
       "       [ 6.86460002e+01],\n",
       "       [ 7.00534504e+01],\n",
       "       [ 2.18712834e+01],\n",
       "       [ 2.43034079e+01],\n",
       "       [ 1.03880934e+02],\n",
       "       [ 7.18503017e+01],\n",
       "       [-9.20217399e+01],\n",
       "       [-1.51774214e+02],\n",
       "       [ 1.61290767e+01],\n",
       "       [-3.36169660e+01],\n",
       "       [-3.54660403e+01],\n",
       "       [ 2.02494817e+01],\n",
       "       [-7.79212517e-01],\n",
       "       [ 4.75853549e+01],\n",
       "       [ 1.38376691e+02],\n",
       "       [ 2.79284706e+00],\n",
       "       [ 5.57373923e+01],\n",
       "       [-1.72018289e+01],\n",
       "       [-7.10086110e+01],\n",
       "       [ 9.71818919e+01],\n",
       "       [ 9.89439839e+01],\n",
       "       [ 8.00715441e+01],\n",
       "       [-1.91857475e+01],\n",
       "       [ 2.83996138e+01],\n",
       "       [-1.49395027e+02],\n",
       "       [-7.08946123e+01],\n",
       "       [ 2.33040657e+01],\n",
       "       [ 1.30750945e+02],\n",
       "       [ 6.06070397e+01],\n",
       "       [-7.13446060e+00],\n",
       "       [ 4.75265570e+01],\n",
       "       [ 3.18792555e+01],\n",
       "       [-5.10784314e+00],\n",
       "       [-4.03160405e+01],\n",
       "       [-2.95332804e+01],\n",
       "       [ 4.08425564e+01],\n",
       "       [-4.23496299e+01],\n",
       "       [-5.86748786e+01],\n",
       "       [-2.55218471e+01],\n",
       "       [ 3.48138529e+01],\n",
       "       [-4.42367359e+01],\n",
       "       [-5.28819324e+01],\n",
       "       [-5.77165575e+01],\n",
       "       [-9.04083460e+01],\n",
       "       [-7.05943048e+01],\n",
       "       [-1.69589491e+01],\n",
       "       [ 5.66819709e+01],\n",
       "       [ 1.02451903e+02],\n",
       "       [ 2.22385262e+01],\n",
       "       [ 5.18436817e+01],\n",
       "       [ 3.59353127e+01],\n",
       "       [ 1.40028258e-01],\n",
       "       [ 3.05268162e+01],\n",
       "       [-2.66669497e+01],\n",
       "       [-1.07604288e+01],\n",
       "       [-3.96603853e+00],\n",
       "       [-7.00414822e+01],\n",
       "       [-7.33390316e+01],\n",
       "       [ 1.16920341e+01],\n",
       "       [ 3.89407595e+01],\n",
       "       [ 5.73096472e+01],\n",
       "       [ 8.73217682e+01],\n",
       "       [ 6.37591858e+01],\n",
       "       [ 9.54350994e+01],\n",
       "       [ 5.03106827e+01],\n",
       "       [-7.74942170e+01],\n",
       "       [ 4.91873589e+01],\n",
       "       [ 2.90601056e+00],\n",
       "       [ 5.67087992e+00],\n",
       "       [-2.90817280e+01],\n",
       "       [ 2.39828519e+01],\n",
       "       [-1.53550637e+01],\n",
       "       [ 6.16899498e+01],\n",
       "       [ 8.33839044e+00],\n",
       "       [-9.20494006e+01],\n",
       "       [-3.72003592e+01],\n",
       "       [ 5.11585232e+01],\n",
       "       [ 4.90394241e+01],\n",
       "       [ 1.57229569e+01],\n",
       "       [-5.09775758e+01],\n",
       "       [-2.49324113e+00],\n",
       "       [-1.10176198e+02],\n",
       "       [-9.06750562e+01],\n",
       "       [ 1.28778941e+00],\n",
       "       [-8.46435899e+01],\n",
       "       [ 1.94591319e+01],\n",
       "       [ 7.26336566e+00],\n",
       "       [-4.00500123e+01],\n",
       "       [-4.42049111e+00],\n",
       "       [-1.45251284e+02],\n",
       "       [-2.15135676e+01],\n",
       "       [-1.34623405e+01],\n",
       "       [-1.65222995e+00],\n",
       "       [ 8.15869894e+01],\n",
       "       [-8.85474823e+01],\n",
       "       [-5.57984865e+01],\n",
       "       [-1.46917206e+01],\n",
       "       [-3.93055231e+01],\n",
       "       [ 9.61359145e+01],\n",
       "       [-1.19186346e+02]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = datasets.make_regression(n_samples=200, n_features=1, noise=0.01)\n",
    "y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b38dd6b5-bf05-4a7b-b1dd-690b2c4d4467",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 29660.294921875\n",
      "Epoch [20/1000], Loss: 20521.984375\n",
      "Epoch [30/1000], Loss: 6815.00341796875\n",
      "Epoch [40/1000], Loss: 1922.5445556640625\n",
      "Epoch [50/1000], Loss: 1664.8763427734375\n",
      "Epoch [60/1000], Loss: 846.9177856445312\n",
      "Epoch [70/1000], Loss: 351.31817626953125\n",
      "Epoch [80/1000], Loss: 180.38546752929688\n",
      "Epoch [90/1000], Loss: 93.06334686279297\n",
      "Epoch [100/1000], Loss: 54.68891143798828\n",
      "Epoch [110/1000], Loss: 42.119869232177734\n",
      "Epoch [120/1000], Loss: 32.17827606201172\n",
      "Epoch [130/1000], Loss: 23.82933235168457\n",
      "Epoch [140/1000], Loss: 18.044002532958984\n",
      "Epoch [150/1000], Loss: 14.715387344360352\n",
      "Epoch [160/1000], Loss: 12.667859077453613\n",
      "Epoch [170/1000], Loss: 11.18124008178711\n",
      "Epoch [180/1000], Loss: 10.054296493530273\n",
      "Epoch [190/1000], Loss: 9.173721313476562\n",
      "Epoch [200/1000], Loss: 8.458874702453613\n",
      "Epoch [210/1000], Loss: 7.865475177764893\n",
      "Epoch [220/1000], Loss: 7.3598127365112305\n",
      "Epoch [230/1000], Loss: 6.919821739196777\n",
      "Epoch [240/1000], Loss: 6.534282684326172\n",
      "Epoch [250/1000], Loss: 6.193149089813232\n",
      "Epoch [260/1000], Loss: 5.890316486358643\n",
      "Epoch [270/1000], Loss: 5.618593692779541\n",
      "Epoch [280/1000], Loss: 5.37487268447876\n",
      "Epoch [290/1000], Loss: 5.153431415557861\n",
      "Epoch [300/1000], Loss: 4.952056884765625\n",
      "Epoch [310/1000], Loss: 4.766959190368652\n",
      "Epoch [320/1000], Loss: 4.597124099731445\n",
      "Epoch [330/1000], Loss: 4.4405927658081055\n",
      "Epoch [340/1000], Loss: 4.296520233154297\n",
      "Epoch [350/1000], Loss: 4.162618637084961\n",
      "Epoch [360/1000], Loss: 4.037661075592041\n",
      "Epoch [370/1000], Loss: 3.920639753341675\n",
      "Epoch [380/1000], Loss: 3.8104305267333984\n",
      "Epoch [390/1000], Loss: 3.7063796520233154\n",
      "Epoch [400/1000], Loss: 3.6077749729156494\n",
      "Epoch [410/1000], Loss: 3.5141260623931885\n",
      "Epoch [420/1000], Loss: 3.4253647327423096\n",
      "Epoch [430/1000], Loss: 3.3410520553588867\n",
      "Epoch [440/1000], Loss: 3.2606005668640137\n",
      "Epoch [450/1000], Loss: 3.183912992477417\n",
      "Epoch [460/1000], Loss: 3.11081862449646\n",
      "Epoch [470/1000], Loss: 3.0411112308502197\n",
      "Epoch [480/1000], Loss: 2.9743521213531494\n",
      "Epoch [490/1000], Loss: 2.9104809761047363\n",
      "Epoch [500/1000], Loss: 2.8492588996887207\n",
      "Epoch [510/1000], Loss: 2.7903172969818115\n",
      "Epoch [520/1000], Loss: 2.7332987785339355\n",
      "Epoch [530/1000], Loss: 2.6786627769470215\n",
      "Epoch [540/1000], Loss: 2.626307725906372\n",
      "Epoch [550/1000], Loss: 2.576232671737671\n",
      "Epoch [560/1000], Loss: 2.5281012058258057\n",
      "Epoch [570/1000], Loss: 2.481886148452759\n",
      "Epoch [580/1000], Loss: 2.4374876022338867\n",
      "Epoch [590/1000], Loss: 2.3948287963867188\n",
      "Epoch [600/1000], Loss: 2.353623390197754\n",
      "Epoch [610/1000], Loss: 2.3136303424835205\n",
      "Epoch [620/1000], Loss: 2.2748947143554688\n",
      "Epoch [630/1000], Loss: 2.237380027770996\n",
      "Epoch [640/1000], Loss: 2.2009358406066895\n",
      "Epoch [650/1000], Loss: 2.1655964851379395\n",
      "Epoch [660/1000], Loss: 2.1313092708587646\n",
      "Epoch [670/1000], Loss: 2.0980939865112305\n",
      "Epoch [680/1000], Loss: 2.0659255981445312\n",
      "Epoch [690/1000], Loss: 2.0347726345062256\n",
      "Epoch [700/1000], Loss: 2.004502773284912\n",
      "Epoch [710/1000], Loss: 1.9751882553100586\n",
      "Epoch [720/1000], Loss: 1.946783423423767\n",
      "Epoch [730/1000], Loss: 1.9192490577697754\n",
      "Epoch [740/1000], Loss: 1.8924590349197388\n",
      "Epoch [750/1000], Loss: 1.8664608001708984\n",
      "Epoch [760/1000], Loss: 1.8412375450134277\n",
      "Epoch [770/1000], Loss: 1.8167608976364136\n",
      "Epoch [780/1000], Loss: 1.7929435968399048\n",
      "Epoch [790/1000], Loss: 1.7697705030441284\n",
      "Epoch [800/1000], Loss: 1.7472410202026367\n",
      "Epoch [810/1000], Loss: 1.7252953052520752\n",
      "Epoch [820/1000], Loss: 1.7039602994918823\n",
      "Epoch [830/1000], Loss: 1.683195948600769\n",
      "Epoch [840/1000], Loss: 1.6629937887191772\n",
      "Epoch [850/1000], Loss: 1.643304705619812\n",
      "Epoch [860/1000], Loss: 1.6241517066955566\n",
      "Epoch [870/1000], Loss: 1.6054850816726685\n",
      "Epoch [880/1000], Loss: 1.5872077941894531\n",
      "Epoch [890/1000], Loss: 1.5692851543426514\n",
      "Epoch [900/1000], Loss: 1.551734209060669\n",
      "Epoch [910/1000], Loss: 1.534578561782837\n",
      "Epoch [920/1000], Loss: 1.5177620649337769\n",
      "Epoch [930/1000], Loss: 1.5012941360473633\n",
      "Epoch [940/1000], Loss: 1.485198974609375\n",
      "Epoch [950/1000], Loss: 1.469454050064087\n",
      "Epoch [960/1000], Loss: 1.4540382623672485\n",
      "Epoch [970/1000], Loss: 1.4389355182647705\n",
      "Epoch [980/1000], Loss: 1.4241231679916382\n",
      "Epoch [990/1000], Loss: 1.4095828533172607\n",
      "Epoch [1000/1000], Loss: 1.395310640335083\n",
      "9.535410165786743\n"
     ]
    }
   ],
   "source": [
    "import nn\n",
    "from tensor import Tensor\n",
    "# Generate the data\n",
    "# n_samples = 200\n",
    "X, y = datasets.make_regression(n_samples=1000, n_features=10,n_targets=10, noise=0.01)\n",
    "import time\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.fc1 = nn.Linear(10, 256)\n",
    "        self.fc2 = nn.Linear(256, 256) # Second hidden layer, 64 neurons\n",
    "        self.output = nn.Linear(256, 10) # Output layer, 1 output value\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x).relu()\n",
    "        x = self.fc2(x).relu()\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "model = MLP()\n",
    "optim = nn.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "x = Tensor(X)\n",
    "y_real = Tensor(y)#.reshape(-1, 1)\n",
    "\n",
    "epochs = 1000\n",
    "start_time = time.time() \n",
    "for epoch in range(epochs):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y_real)\n",
    "\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.data}')\n",
    "end_time = time.time()  # Record the end time\n",
    "elapsed_time = end_time - start_time  # Calculate the elapsed time   \n",
    "\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "705f10ad-612c-43ab-ba78-60d82abadc62",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (200,1) doesn't match the broadcast shape (200,200)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \n\u001b[1;32m      2\u001b[0m b \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m200\u001b[39m, )\n\u001b[0;32m----> 4\u001b[0m a \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m b\n",
      "\u001b[0;31mValueError\u001b[0m: non-broadcastable output operand with shape (200,1) doesn't match the broadcast shape (200,200)"
     ]
    }
   ],
   "source": [
    "a = np.random.randn(200, 1)  \n",
    "b = np.random.randn(200, )\n",
    "\n",
    "a += b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07247f0f-f291-4104-b0e7-885d154a2293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 10)\n",
      "Epoch [10/1000], Loss: 28299.5938\n",
      "Epoch [20/1000], Loss: 9023.1807\n",
      "Epoch [30/1000], Loss: 2652.9724\n",
      "Epoch [40/1000], Loss: 1187.3018\n",
      "Epoch [50/1000], Loss: 514.5989\n",
      "Epoch [60/1000], Loss: 186.0085\n",
      "Epoch [70/1000], Loss: 103.5073\n",
      "Epoch [80/1000], Loss: 57.4446\n",
      "Epoch [90/1000], Loss: 35.7552\n",
      "Epoch [100/1000], Loss: 22.7661\n",
      "Epoch [110/1000], Loss: 17.5981\n",
      "Epoch [120/1000], Loss: 15.4110\n",
      "Epoch [130/1000], Loss: 13.2682\n",
      "Epoch [140/1000], Loss: 11.4617\n",
      "Epoch [150/1000], Loss: 9.8916\n",
      "Epoch [160/1000], Loss: 8.5199\n",
      "Epoch [170/1000], Loss: 7.4321\n",
      "Epoch [180/1000], Loss: 6.6498\n",
      "Epoch [190/1000], Loss: 6.1097\n",
      "Epoch [200/1000], Loss: 5.7118\n",
      "Epoch [210/1000], Loss: 5.3878\n",
      "Epoch [220/1000], Loss: 5.1087\n",
      "Epoch [230/1000], Loss: 4.8652\n",
      "Epoch [240/1000], Loss: 4.6494\n",
      "Epoch [250/1000], Loss: 4.4557\n",
      "Epoch [260/1000], Loss: 4.2799\n",
      "Epoch [270/1000], Loss: 4.1198\n",
      "Epoch [280/1000], Loss: 3.9730\n",
      "Epoch [290/1000], Loss: 3.8377\n",
      "Epoch [300/1000], Loss: 3.7124\n",
      "Epoch [310/1000], Loss: 3.5958\n",
      "Epoch [320/1000], Loss: 3.4865\n",
      "Epoch [330/1000], Loss: 3.3838\n",
      "Epoch [340/1000], Loss: 3.2871\n",
      "Epoch [350/1000], Loss: 3.1957\n",
      "Epoch [360/1000], Loss: 3.1093\n",
      "Epoch [370/1000], Loss: 3.0278\n",
      "Epoch [380/1000], Loss: 2.9505\n",
      "Epoch [390/1000], Loss: 2.8771\n",
      "Epoch [400/1000], Loss: 2.8075\n",
      "Epoch [410/1000], Loss: 2.7413\n",
      "Epoch [420/1000], Loss: 2.6781\n",
      "Epoch [430/1000], Loss: 2.6178\n",
      "Epoch [440/1000], Loss: 2.5600\n",
      "Epoch [450/1000], Loss: 2.5049\n",
      "Epoch [460/1000], Loss: 2.4523\n",
      "Epoch [470/1000], Loss: 2.4017\n",
      "Epoch [480/1000], Loss: 2.3531\n",
      "Epoch [490/1000], Loss: 2.3065\n",
      "Epoch [500/1000], Loss: 2.2618\n",
      "Epoch [510/1000], Loss: 2.2189\n",
      "Epoch [520/1000], Loss: 2.1777\n",
      "Epoch [530/1000], Loss: 2.1380\n",
      "Epoch [540/1000], Loss: 2.0998\n",
      "Epoch [550/1000], Loss: 2.0629\n",
      "Epoch [560/1000], Loss: 2.0273\n",
      "Epoch [570/1000], Loss: 1.9929\n",
      "Epoch [580/1000], Loss: 1.9595\n",
      "Epoch [590/1000], Loss: 1.9271\n",
      "Epoch [600/1000], Loss: 1.8957\n",
      "Epoch [610/1000], Loss: 1.8652\n",
      "Epoch [620/1000], Loss: 1.8356\n",
      "Epoch [630/1000], Loss: 1.8069\n",
      "Epoch [640/1000], Loss: 1.7792\n",
      "Epoch [650/1000], Loss: 1.7524\n",
      "Epoch [660/1000], Loss: 1.7264\n",
      "Epoch [670/1000], Loss: 1.7011\n",
      "Epoch [680/1000], Loss: 1.6764\n",
      "Epoch [690/1000], Loss: 1.6524\n",
      "Epoch [700/1000], Loss: 1.6289\n",
      "Epoch [710/1000], Loss: 1.6061\n",
      "Epoch [720/1000], Loss: 1.5839\n",
      "Epoch [730/1000], Loss: 1.5624\n",
      "Epoch [740/1000], Loss: 1.5416\n",
      "Epoch [750/1000], Loss: 1.5213\n",
      "Epoch [760/1000], Loss: 1.5014\n",
      "Epoch [770/1000], Loss: 1.4821\n",
      "Epoch [780/1000], Loss: 1.4631\n",
      "Epoch [790/1000], Loss: 1.4446\n",
      "Epoch [800/1000], Loss: 1.4266\n",
      "Epoch [810/1000], Loss: 1.4091\n",
      "Epoch [820/1000], Loss: 1.3919\n",
      "Epoch [830/1000], Loss: 1.3751\n",
      "Epoch [840/1000], Loss: 1.3587\n",
      "Epoch [850/1000], Loss: 1.3426\n",
      "Epoch [860/1000], Loss: 1.3269\n",
      "Epoch [870/1000], Loss: 1.3116\n",
      "Epoch [880/1000], Loss: 1.2966\n",
      "Epoch [890/1000], Loss: 1.2820\n",
      "Epoch [900/1000], Loss: 1.2678\n",
      "Epoch [910/1000], Loss: 1.2538\n",
      "Epoch [920/1000], Loss: 1.2401\n",
      "Epoch [930/1000], Loss: 1.2267\n",
      "Epoch [940/1000], Loss: 1.2135\n",
      "Epoch [950/1000], Loss: 1.2006\n",
      "Epoch [960/1000], Loss: 1.1879\n",
      "Epoch [970/1000], Loss: 1.1754\n",
      "Epoch [980/1000], Loss: 1.1632\n",
      "Epoch [990/1000], Loss: 1.1513\n",
      "Epoch [1000/1000], Loss: 1.1395\n",
      "1.6494381427764893\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from sklearn import datasets\n",
    "\n",
    "X, y = datasets.make_regression(n_samples=1000, n_features=10, n_targets=10, noise=0.01)\n",
    "print(y.shape)\n",
    "\n",
    "grads = []\n",
    "# Define the neural network architecture\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.output = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the network\n",
    "net = SimpleMLP()\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Example training loop\n",
    "def train(model, criterion, optimizer, x_train, y_train, epochs=100):\n",
    "    model.train()\n",
    "    start_time = time.time() \n",
    "    for epoch in range(epochs):\n",
    "        inputs = torch.tensor(x_train, dtype=torch.float32)\n",
    "        targets = torch.tensor(y_train, dtype=torch.float32)\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "    end_time = time.time()\n",
    "    diff = end_time - start_time\n",
    "    print(diff)\n",
    "\n",
    "# Assuming you have `xs` and `ys` from your dataset\n",
    "train(net, criterion, optimizer, X, y, epochs=1000)\n",
    "# plt.plot(np.arange(0, len(grads), 1), grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f49f4ad-2e75-4480-bcc3-49b30dbedabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 1])\n",
      "Epoch [10/500], Loss: 0.7151\n",
      "Epoch [20/500], Loss: 0.7013\n",
      "Epoch [30/500], Loss: 0.6858\n",
      "Epoch [40/500], Loss: 0.6704\n",
      "Epoch [50/500], Loss: 0.6558\n",
      "Epoch [60/500], Loss: 0.6418\n",
      "Epoch [70/500], Loss: 0.6285\n",
      "Epoch [80/500], Loss: 0.6156\n",
      "Epoch [90/500], Loss: 0.6032\n",
      "Epoch [100/500], Loss: 0.5912\n",
      "Epoch [110/500], Loss: 0.5794\n",
      "Epoch [120/500], Loss: 0.5678\n",
      "Epoch [130/500], Loss: 0.5564\n",
      "Epoch [140/500], Loss: 0.5452\n",
      "Epoch [150/500], Loss: 0.5342\n",
      "Epoch [160/500], Loss: 0.5233\n",
      "Epoch [170/500], Loss: 0.5127\n",
      "Epoch [180/500], Loss: 0.5022\n",
      "Epoch [190/500], Loss: 0.4918\n",
      "Epoch [200/500], Loss: 0.4818\n",
      "Epoch [210/500], Loss: 0.4719\n",
      "Epoch [220/500], Loss: 0.4623\n",
      "Epoch [230/500], Loss: 0.4529\n",
      "Epoch [240/500], Loss: 0.4438\n",
      "Epoch [250/500], Loss: 0.4350\n",
      "Epoch [260/500], Loss: 0.4265\n",
      "Epoch [270/500], Loss: 0.4182\n",
      "Epoch [280/500], Loss: 0.4103\n",
      "Epoch [290/500], Loss: 0.4026\n",
      "Epoch [300/500], Loss: 0.3952\n",
      "Epoch [310/500], Loss: 0.3882\n",
      "Epoch [320/500], Loss: 0.3814\n",
      "Epoch [330/500], Loss: 0.3749\n",
      "Epoch [340/500], Loss: 0.3687\n",
      "Epoch [350/500], Loss: 0.3628\n",
      "Epoch [360/500], Loss: 0.3572\n",
      "Epoch [370/500], Loss: 0.3518\n",
      "Epoch [380/500], Loss: 0.3467\n",
      "Epoch [390/500], Loss: 0.3418\n",
      "Epoch [400/500], Loss: 0.3372\n",
      "Epoch [410/500], Loss: 0.3328\n",
      "Epoch [420/500], Loss: 0.3286\n",
      "Epoch [430/500], Loss: 0.3246\n",
      "Epoch [440/500], Loss: 0.3208\n",
      "Epoch [450/500], Loss: 0.3172\n",
      "Epoch [460/500], Loss: 0.3137\n",
      "Epoch [470/500], Loss: 0.3104\n",
      "Epoch [480/500], Loss: 0.3073\n",
      "Epoch [490/500], Loss: 0.3043\n",
      "Epoch [500/500], Loss: 0.3015\n",
      "Accuracy: 88.50%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10, n_clusters_per_class=1, random_state=42)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "print(y_train.shape)\n",
    "\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 16)\n",
    "        self.fc2 = nn.Linear(16, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_dim = X_train.shape[1]\n",
    "model = SimpleNN(input_dim)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "\n",
    "num_epochs = 500\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    predicted = outputs.round()\n",
    "    # accuracy = (predicted.eq(y_test).sum().item()) / y_test.size(0)\n",
    "    accuracy = (predicted == y_test).sum().item() / y_test.size(0)\n",
    "    print(f'Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20512973-e1f5-43ab-943a-6da693b39909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False],\n",
       "        [False]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(X_test)\n",
    "predicted = outputs.round()\n",
    "\n",
    "outputs == predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "568fc1b9-4f8a-4f78-9b57-391344bccb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/500], Loss: 0.6653\n",
      "Epoch [20/500], Loss: 0.5871\n",
      "Epoch [30/500], Loss: 0.5244\n",
      "Epoch [40/500], Loss: 0.4800\n",
      "Epoch [50/500], Loss: 0.4477\n",
      "Epoch [60/500], Loss: 0.4232\n",
      "Epoch [70/500], Loss: 0.4037\n",
      "Epoch [80/500], Loss: 0.3876\n",
      "Epoch [90/500], Loss: 0.3742\n",
      "Epoch [100/500], Loss: 0.3627\n",
      "Epoch [110/500], Loss: 0.3527\n",
      "Epoch [120/500], Loss: 0.3439\n",
      "Epoch [130/500], Loss: 0.3361\n",
      "Epoch [140/500], Loss: 0.3291\n",
      "Epoch [150/500], Loss: 0.3227\n",
      "Epoch [160/500], Loss: 0.3169\n",
      "Epoch [170/500], Loss: 0.3116\n",
      "Epoch [180/500], Loss: 0.3068\n",
      "Epoch [190/500], Loss: 0.3024\n",
      "Epoch [200/500], Loss: 0.2984\n",
      "Epoch [210/500], Loss: 0.2947\n",
      "Epoch [220/500], Loss: 0.2912\n",
      "Epoch [230/500], Loss: 0.2880\n",
      "Epoch [240/500], Loss: 0.2850\n",
      "Epoch [250/500], Loss: 0.2822\n",
      "Epoch [260/500], Loss: 0.2796\n",
      "Epoch [270/500], Loss: 0.2771\n",
      "Epoch [280/500], Loss: 0.2747\n",
      "Epoch [290/500], Loss: 0.2725\n",
      "Epoch [300/500], Loss: 0.2704\n",
      "Epoch [310/500], Loss: 0.2684\n",
      "Epoch [320/500], Loss: 0.2665\n",
      "Epoch [330/500], Loss: 0.2647\n",
      "Epoch [340/500], Loss: 0.2629\n",
      "Epoch [350/500], Loss: 0.2613\n",
      "Epoch [360/500], Loss: 0.2597\n",
      "Epoch [370/500], Loss: 0.2582\n",
      "Epoch [380/500], Loss: 0.2567\n",
      "Epoch [390/500], Loss: 0.2554\n",
      "Epoch [400/500], Loss: 0.2540\n",
      "Epoch [410/500], Loss: 0.2528\n",
      "Epoch [420/500], Loss: 0.2516\n",
      "Epoch [430/500], Loss: 0.2505\n",
      "Epoch [440/500], Loss: 0.2494\n",
      "Epoch [450/500], Loss: 0.2484\n",
      "Epoch [460/500], Loss: 0.2474\n",
      "Epoch [470/500], Loss: 0.2464\n",
      "Epoch [480/500], Loss: 0.2454\n",
      "Epoch [490/500], Loss: 0.2445\n",
      "Epoch [500/500], Loss: 0.2436\n",
      "Accuracy: 91.00%\n"
     ]
    }
   ],
   "source": [
    "from tensor import Tensor\n",
    "import nn\n",
    "import numpy as np\n",
    "\n",
    "# Datastuff\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10, n_clusters_per_class=1, random_state=42)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = Tensor(X_train)\n",
    "X_test = Tensor(X_test)\n",
    "y_train = Tensor(y_train).unsqueeze(1)\n",
    "y_test = Tensor(y_test).unsqueeze(1)\n",
    "\n",
    "\n",
    "class NanoNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 16)\n",
    "        self.fc2 = nn.Linear(16, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x).relu()\n",
    "        x = self.fc2(x).sigmoid()\n",
    "        return x\n",
    "\n",
    "# # Initialize the model, loss function, and optimizer\n",
    "input_dim = X_train.shape[1]\n",
    "model = NanoNN(input_dim)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = nn.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "\n",
    "\n",
    "num_epochs = 500\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# model.eval()\n",
    "outputs = model(X_test)  # Ensure model outputs a NumPy array\n",
    "predicted = outputs.round()\n",
    "\n",
    "# # Calculate accuracy\n",
    "accuracy = (predicted == y_test).sum() / y_test.size\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500c98b4-707a-46ea-9bdb-9652ef05176f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7870586d-3605-4659-8240-779c07657329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.]\n",
       " [1.]\n",
       " [0.]\n",
       " [0.]\n",
       " [1.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [1.]\n",
       " [1.]\n",
       " [0.]\n",
       " [0.]\n",
       " [1.]\n",
       " [0.]\n",
       " [1.]\n",
       " [0.]\n",
       " [1.]\n",
       " [1.]\n",
       " [1.]\n",
       " [0.]\n",
       " [0.]\n",
       " [1.]\n",
       " [1.]\n",
       " [1.]\n",
       " [0.]\n",
       " [0.]\n",
       " [1.]\n",
       " [1.]\n",
       " [1.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [1.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [1.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [1.]\n",
       " [0.]\n",
       " [1.]\n",
       " [1.]\n",
       " [1.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [1.]\n",
       " [1.]\n",
       " [0.]\n",
       " [0.]\n",
       " [1.]\n",
       " [1.]\n",
       " [1.]\n",
       " [1.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [1.]\n",
       " [1.]\n",
       " [1.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [1.]\n",
       " [1.]\n",
       " [1.]\n",
       " [1.]\n",
       " [1.]\n",
       " [1.]\n",
       " [0.]\n",
       " [0.]\n",
       " [1.]\n",
       " [1.]\n",
       " [0.]\n",
       " [1.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [1.]\n",
       " [1.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [1.]\n",
       " [0.]\n",
       " [0.]\n",
       " [1.]\n",
       " [0.]\n",
       " [1.]\n",
       " [1.]\n",
       " [1.]\n",
       " [0.]\n",
       " [1.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [1.]\n",
       " [1.]\n",
       " [1.]\n",
       " [1.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [1.]\n",
       " [0.]\n",
       " [0.]\n",
       " [1.]\n",
       " [0.]\n",
       " [1.]\n",
       " [1.]\n",
       " [0.]\n",
       " [1.]\n",
       " [0.]\n",
       " [1.]\n",
       " [0.]\n",
       " [0.]\n",
       " [1.]\n",
       " [0.]\n",
       " [1.]\n",
       " [0.]\n",
       " [1.]\n",
       " [1.]\n",
       " [0.]\n",
       " [0.]\n",
       " [1.]\n",
       " [0.]\n",
       " [1.]\n",
       " [0.]\n",
       " [1.]\n",
       " [1.]\n",
       " [0.]\n",
       " [1.]\n",
       " [1.]\n",
       " [0.]\n",
       " [0.]\n",
       " [1.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [1.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [1.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [1.]\n",
       " [1.]\n",
       " [1.]\n",
       " [0.]\n",
       " [0.]\n",
       " [0.]\n",
       " [1.]\n",
       " [0.]\n",
       " [0.]\n",
       " [1.]\n",
       " [1.]\n",
       " [1.]\n",
       " [1.]\n",
       " [0.]\n",
       " [0.]\n",
       " [1.]\n",
       " [0.]\n",
       " [1.]\n",
       " [1.]\n",
       " [1.]\n",
       " [0.]\n",
       " [0.]\n",
       " [1.]\n",
       " [0.]\n",
       " [1.]\n",
       " [0.]\n",
       " [0.]\n",
       " [1.]\n",
       " [0.]\n",
       " [0.]\n",
       " [1.]\n",
       " [1.]\n",
       " [0.]\n",
       " [0.]\n",
       " [1.]\n",
       " [1.]\n",
       " [0.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12449089-0d74-4a90-8682-9dd6533bb685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted == outputs.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e1613b6-7f4b-4b97-8a19-0be55c4641db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800])\n",
      "Epoch [100/5000], Loss: 0.3175\n",
      "Epoch [200/5000], Loss: 0.2877\n",
      "Epoch [300/5000], Loss: 0.2762\n",
      "Epoch [400/5000], Loss: 0.2676\n",
      "Epoch [500/5000], Loss: 0.2603\n",
      "Epoch [600/5000], Loss: 0.2538\n",
      "Epoch [700/5000], Loss: 0.2477\n",
      "Epoch [800/5000], Loss: 0.2413\n",
      "Epoch [900/5000], Loss: 0.2348\n",
      "Epoch [1000/5000], Loss: 0.2289\n",
      "Epoch [1100/5000], Loss: 0.2227\n",
      "Epoch [1200/5000], Loss: 0.2169\n",
      "Epoch [1300/5000], Loss: 0.2112\n",
      "Epoch [1400/5000], Loss: 0.2060\n",
      "Epoch [1500/5000], Loss: 0.2012\n",
      "Epoch [1600/5000], Loss: 0.1966\n",
      "Epoch [1700/5000], Loss: 0.1923\n",
      "Epoch [1800/5000], Loss: 0.1882\n",
      "Epoch [1900/5000], Loss: 0.1844\n",
      "Epoch [2000/5000], Loss: 0.1807\n",
      "Epoch [2100/5000], Loss: 0.1774\n",
      "Epoch [2200/5000], Loss: 0.1744\n",
      "Epoch [2300/5000], Loss: 0.1712\n",
      "Epoch [2400/5000], Loss: 0.1684\n",
      "Epoch [2500/5000], Loss: 0.1657\n",
      "Epoch [2600/5000], Loss: 0.1630\n",
      "Epoch [2700/5000], Loss: 0.1602\n",
      "Epoch [2800/5000], Loss: 0.1575\n",
      "Epoch [2900/5000], Loss: 0.1548\n",
      "Epoch [3000/5000], Loss: 0.1519\n",
      "Epoch [3100/5000], Loss: 0.1489\n",
      "Epoch [3200/5000], Loss: 0.1458\n",
      "Epoch [3300/5000], Loss: 0.1431\n",
      "Epoch [3400/5000], Loss: 0.1406\n",
      "Epoch [3500/5000], Loss: 0.1382\n",
      "Epoch [3600/5000], Loss: 0.1358\n",
      "Epoch [3700/5000], Loss: 0.1336\n",
      "Epoch [3800/5000], Loss: 0.1312\n",
      "Epoch [3900/5000], Loss: 0.1289\n",
      "Epoch [4000/5000], Loss: 0.1266\n",
      "Epoch [4100/5000], Loss: 0.1243\n",
      "Epoch [4200/5000], Loss: 0.1220\n",
      "Epoch [4300/5000], Loss: 0.1196\n",
      "Epoch [4400/5000], Loss: 0.1176\n",
      "Epoch [4500/5000], Loss: 0.1158\n",
      "Epoch [4600/5000], Loss: 0.1140\n",
      "Epoch [4700/5000], Loss: 0.1123\n",
      "Epoch [4800/5000], Loss: 0.1106\n",
      "Epoch [4900/5000], Loss: 0.1090\n",
      "Epoch [5000/5000], Loss: 0.1074\n",
      "Accuracy: 88.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate synthetic dataset with 3 classes\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10, n_classes=3, n_clusters_per_class=1, random_state=42)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "print(y_train.shape)\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 16)\n",
    "        self.fc2 = nn.Linear(16, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_dim = X_train.shape[1]\n",
    "num_classes = 3\n",
    "model = SimpleNN(input_dim, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "num_epochs = 5000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    accuracy = (predicted == y_test).sum().item() / y_test.size(0)\n",
    "    print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef1b9d09-2e99-4513-baf0-08a74bcc353b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/7000], Loss: 1.0741\n",
      "Epoch [200/7000], Loss: 1.0407\n",
      "Epoch [300/7000], Loss: 1.0094\n",
      "Epoch [400/7000], Loss: 0.9797\n",
      "Epoch [500/7000], Loss: 0.9512\n",
      "Epoch [600/7000], Loss: 0.9235\n",
      "Epoch [700/7000], Loss: 0.8965\n",
      "Epoch [800/7000], Loss: 0.8702\n",
      "Epoch [900/7000], Loss: 0.8442\n",
      "Epoch [1000/7000], Loss: 0.8189\n",
      "Epoch [1100/7000], Loss: 0.7943\n",
      "Epoch [1200/7000], Loss: 0.7703\n",
      "Epoch [1300/7000], Loss: 0.7471\n",
      "Epoch [1400/7000], Loss: 0.7248\n",
      "Epoch [1500/7000], Loss: 0.7034\n",
      "Epoch [1600/7000], Loss: 0.6829\n",
      "Epoch [1700/7000], Loss: 0.6634\n",
      "Epoch [1800/7000], Loss: 0.6449\n",
      "Epoch [1900/7000], Loss: 0.6274\n",
      "Epoch [2000/7000], Loss: 0.6107\n",
      "Epoch [2100/7000], Loss: 0.5949\n",
      "Epoch [2200/7000], Loss: 0.5801\n",
      "Epoch [2300/7000], Loss: 0.5660\n",
      "Epoch [2400/7000], Loss: 0.5527\n",
      "Epoch [2500/7000], Loss: 0.5401\n",
      "Epoch [2600/7000], Loss: 0.5282\n",
      "Epoch [2700/7000], Loss: 0.5170\n",
      "Epoch [2800/7000], Loss: 0.5064\n",
      "Epoch [2900/7000], Loss: 0.4964\n",
      "Epoch [3000/7000], Loss: 0.4869\n",
      "Epoch [3100/7000], Loss: 0.4779\n",
      "Epoch [3200/7000], Loss: 0.4695\n",
      "Epoch [3300/7000], Loss: 0.4615\n",
      "Epoch [3400/7000], Loss: 0.4540\n",
      "Epoch [3500/7000], Loss: 0.4469\n",
      "Epoch [3600/7000], Loss: 0.4402\n",
      "Epoch [3700/7000], Loss: 0.4338\n",
      "Epoch [3800/7000], Loss: 0.4278\n",
      "Epoch [3900/7000], Loss: 0.4222\n",
      "Epoch [4000/7000], Loss: 0.4168\n",
      "Epoch [4100/7000], Loss: 0.4118\n",
      "Epoch [4200/7000], Loss: 0.4070\n",
      "Epoch [4300/7000], Loss: 0.4025\n",
      "Epoch [4400/7000], Loss: 0.3982\n",
      "Epoch [4500/7000], Loss: 0.3942\n",
      "Epoch [4600/7000], Loss: 0.3904\n",
      "Epoch [4700/7000], Loss: 0.3868\n",
      "Epoch [4800/7000], Loss: 0.3833\n",
      "Epoch [4900/7000], Loss: 0.3801\n",
      "Epoch [5000/7000], Loss: 0.3770\n",
      "Epoch [5100/7000], Loss: 0.3740\n",
      "Epoch [5200/7000], Loss: 0.3713\n",
      "Epoch [5300/7000], Loss: 0.3686\n",
      "Epoch [5400/7000], Loss: 0.3661\n",
      "Epoch [5500/7000], Loss: 0.3636\n",
      "Epoch [5600/7000], Loss: 0.3613\n",
      "Epoch [5700/7000], Loss: 0.3591\n",
      "Epoch [5800/7000], Loss: 0.3570\n",
      "Epoch [5900/7000], Loss: 0.3550\n",
      "Epoch [6000/7000], Loss: 0.3530\n",
      "Epoch [6100/7000], Loss: 0.3512\n",
      "Epoch [6200/7000], Loss: 0.3494\n",
      "Epoch [6300/7000], Loss: 0.3477\n",
      "Epoch [6400/7000], Loss: 0.3461\n",
      "Epoch [6500/7000], Loss: 0.3445\n",
      "Epoch [6600/7000], Loss: 0.3430\n",
      "Epoch [6700/7000], Loss: 0.3415\n",
      "Epoch [6800/7000], Loss: 0.3402\n",
      "Epoch [6900/7000], Loss: 0.3388\n",
      "Epoch [7000/7000], Loss: 0.3375\n",
      "Accuracy: 85.50%\n"
     ]
    }
   ],
   "source": [
    "from tensor import Tensor\n",
    "import nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate synthetic dataset with 3 classes\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10, n_classes=3, n_clusters_per_class=1, random_state=42)\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "\n",
    "X_train = Tensor(X_train)\n",
    "X_test = Tensor(X_test)\n",
    "y_train = Tensor(y_train, dtype=np.int64)\n",
    "y_test = Tensor(y_test, dtype=np.int64)\n",
    "\n",
    "class NanoNN(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 32)\n",
    "        self.fc2 = nn.Linear(32, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x).relu()\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_dim = X_train.shape[1]\n",
    "num_classes = 3\n",
    "model = NanoNN(input_dim, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = nn.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "num_epochs = 7000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# model.eval()\n",
    "outputs = model(X_test)  # Ensure model outputs a NumPy array\n",
    "predicted = outputs.argmax(1)\n",
    "\n",
    "# # Calculate accuracy\n",
    "accuracy = (predicted == y_test).sum() / y_test.size\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7c952b52-cf0e-46ac-b695-320875e9c8b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "259916450.20270044"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mass_earth = 5.972e24\n",
    "mass_sun = 1.989e30\n",
    "# radius_sun = 6.96e8\n",
    "dist_sun = 1.5e11\n",
    "\n",
    "import math\n",
    "\n",
    "dist_sun * math.sqrt(mass_earth/mass_sun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1301d184-557c-428e-99ab-7d8e20a0bb4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "259916450.20270044"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "dist_sun * math.sqrt(mass_earth/mass_sun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9190b903-71fe-43d3-aaec-bb67884a44ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260000000.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2.6e8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "31d11114-8f34-4527-95be-64dbd23e3057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500418854.037184"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mass_earth = 5.972e24  # kg\n",
    "mass_sun = 1.989e30  # kg\n",
    "dist_sun = 1.5e11  # meters\n",
    "\n",
    "# Calculate the Hill sphere radius\n",
    "hill_radius = dist_sun * (mass_earth / (3 * mass_sun)) ** (1/3)\n",
    "hill_radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "13a03d1b-92bb-404b-bd5e-cb9584f7bad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2163978447.5162625"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the distance to the L1 point\n",
    "l1_distance = dist_sun * (mass_earth / mass_sun) ** (1/3)\n",
    "l1_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d68e72d-c29c-4576-a21b-e560b61ec40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Plannets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e2418fe-132f-46f4-9a88-b70d19e359c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.011754989624023438, 0.010983943939208984)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Define the size for the test matrix\n",
    "size = 1000\n",
    "\n",
    "# Generate large value numbers\n",
    "large_numbers = np.random.uniform(1e5, 1e6, (size, size))\n",
    "\n",
    "# Generate small value numbers\n",
    "small_numbers = np.random.uniform(0, 1, (size, size))\n",
    "\n",
    "# Function to perform matrix multiplication and measure time\n",
    "def measure_time(matrix):\n",
    "    start_time = time.time()\n",
    "    result = np.dot(matrix, matrix)\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time\n",
    "\n",
    "# Measure time for large values\n",
    "time_large_values = measure_time(large_numbers)\n",
    "\n",
    "# Measure time for small values\n",
    "time_small_values = measure_time(small_numbers)\n",
    "\n",
    "time_large_values, time_small_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e982a69-1d61-4f8b-873f-8a8428c9c98a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.52153284e+43, 1.72972536e+43, 2.00162669e+43, ...,\n",
       "        5.53988930e+43, 6.31928095e+43, 4.55621353e+43],\n",
       "       [6.48176150e+43, 3.72406090e+43, 2.63894987e+43, ...,\n",
       "        8.96039132e+43, 2.54764489e+43, 5.57717276e+43],\n",
       "       [9.24847882e+43, 1.05483536e+43, 7.06977006e+43, ...,\n",
       "        1.92481684e+43, 4.71791661e+43, 2.90068648e+43],\n",
       "       ...,\n",
       "       [5.16149970e+43, 4.87128750e+43, 3.22942296e+43, ...,\n",
       "        1.38691507e+43, 3.43835651e+43, 1.91900818e+43],\n",
       "       [3.99001210e+43, 8.28701339e+43, 7.74557114e+43, ...,\n",
       "        1.69020224e+43, 1.88477685e+43, 8.60494228e+43],\n",
       "       [2.15029325e+43, 7.67546629e+43, 8.35709906e+43, ...,\n",
       "        2.74099705e+43, 2.43549143e+43, 2.77306975e+43]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fac89884-2fe3-4310-b09a-6def5195bed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.013701915740966797, 0.01443791389465332)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Define the size for the test array\n",
    "size = 10000000\n",
    "\n",
    "# Generate large value numbers\n",
    "large_values = np.random.uniform(1e5, 1e6, size)\n",
    "\n",
    "# Generate small value numbers\n",
    "small_values = np.random.uniform(0, 1, size)\n",
    "\n",
    "# Function to perform element-wise multiplication and measure time\n",
    "def measure_time(array1, array2):\n",
    "    start_time = time.time()\n",
    "    result = np.multiply(array1, array2)\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time\n",
    "\n",
    "# Measure time for large values\n",
    "time_large_values = measure_time(large_values, large_values)\n",
    "\n",
    "# Measure time for small values\n",
    "time_small_values = measure_time(small_values, small_values)\n",
    "\n",
    "time_large_values, time_small_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a42b039-2a38-428c-93ca-e9a5223ffc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "out_features = 5\n",
    "in_features = 10\n",
    "\n",
    "my_tensor = torch.empty((out_features, in_features, 2))\n",
    "\n",
    "print(my_tensor.dim())\n",
    "\n",
    "def _calculate_fan_in_and_fan_out(tensor):\n",
    "    dimensions = tensor.dim()\n",
    "    if dimensions < 2:\n",
    "        raise ValueError(\n",
    "            \"Fan in and fan out can not be computed for tensor with fewer than 2 dimensions\"\n",
    "        )\n",
    "\n",
    "    num_input_fmaps = tensor.size(1)\n",
    "    num_output_fmaps = tensor.size(0)\n",
    "    receptive_field_size = 1\n",
    "    if tensor.dim() > 2:\n",
    "        # math.prod is not always available, accumulate the product manually\n",
    "        # we could use functools.reduce but that is not supported by TorchScript\n",
    "        for s in tensor.shape[2:]:\n",
    "            receptive_field_size *= s\n",
    "    fan_in = num_input_fmaps * receptive_field_size\n",
    "    fan_out = num_output_fmaps * receptive_field_size\n",
    "\n",
    "    return fan_in, fan_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1cea00b-8e50-4db8-b27c-7806ac09884c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95a625a6-e40b-4854-aaf0-87774daaabde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_calculate_fan_in_and_fan_out(my_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "910fbfba-c8c7-441e-896d-4eed6be55eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.random.randn(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79d11157-664c-4dcc-93dd-d628b7c571b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "079ae9fd-9236-4fb5-a100-99f75cbd646a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c17d938c-4d02-4f8e-b841-6625c0335fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import operator\n",
    "\n",
    "def _calculate_fan_in_and_fan_out(tensor):\n",
    "    dimensions = tensor.dim()\n",
    "    if dimensions < 2:\n",
    "        raise ValueError(\n",
    "            \"Fan in and fan out can not be computed for tensor with fewer than 2 dimensions\"\n",
    "        )\n",
    "\n",
    "    num_input_fmaps = tensor.size(1)\n",
    "    num_output_fmaps = tensor.size(0)\n",
    "    \n",
    "    # Use functools.reduce to compute the product of the receptive field dimensions\n",
    "    receptive_field_size = functools.reduce(operator.mul, tensor.shape[2:], 1) if tensor.dim() > 2 else 1\n",
    "    \n",
    "    fan_in = num_input_fmaps * receptive_field_size\n",
    "    fan_out = num_output_fmaps * receptive_field_size\n",
    "\n",
    "    return fan_in, fan_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "050fd5f1-9651-4d19-aa7a-eab846392956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensor import Tensor\n",
    "\n",
    "x = Tensor.normal((5, 10, 2), 0, 1)\n",
    "_calculate_fan_in_and_fan_out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b3a04658-a115-4d0e-8487-96786f323588",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input must be 1- or 2-d.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# adagrad update\u001b[39;00m\n\u001b[1;32m      3\u001b[0m params \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m G \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiag\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# params = params * (0.01/\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ptb/lib/python3.10/site-packages/numpy/lib/twodim_base.py:303\u001b[0m, in \u001b[0;36mdiag\u001b[0;34m(v, k)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m diagonal(v, k)\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput must be 1- or 2-d.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Input must be 1- or 2-d."
     ]
    }
   ],
   "source": [
    "# adagrad update\n",
    "\n",
    "params = np.random.randn(3, 3)\n",
    "grad = np.random.randn(3, 3)\n",
    "sum_of_squared_gradients = \n",
    "epsilon = 1e-8 # smoothing term\n",
    "lr = 0.01\n",
    "grad = np.random.randn(3, 3)\n",
    "params = params * (0.01/np.sqrt(G + epsilon)) * grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6a762d80-e1f7-4298-a65e-3b0c8b3962e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input must be 1- or 2-d.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m G \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiag\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m G\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ptb/lib/python3.10/site-packages/numpy/lib/twodim_base.py:303\u001b[0m, in \u001b[0;36mdiag\u001b[0;34m(v, k)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m diagonal(v, k)\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput must be 1- or 2-d.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Input must be 1- or 2-d."
     ]
    }
   ],
   "source": [
    "G = np.diag(1)\n",
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b4090c2d-1c9b-4fc1-a03d-6403f7bc8da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "69507073-d9aa-4ece-a899-acc213c8bc69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = np.zeros_like(x)\n",
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5595ca81-03ce-4ac3-abcb-f46f729de683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.47275351, 3.07034454],\n",
       "       [2.10455004, 3.67815118]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G += x**2\n",
    "np.sqrt(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9866c1f8-06e7-4fa0-8e28-160422c5ca6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.16900291,  9.42701557],\n",
       "       [ 4.42913089, 13.52879607]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe54a56-0bc2-46c2-add4-6c0d64e5f76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_time = 2.3\n",
    "curr = 1.3\n",
    "\n",
    "import math\n",
    "\n",
    "math.sqrt(prev_time)/math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "360e57bb-468d-475c-832b-a55b273bf851",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 3.1\n",
    "frac = 0.9\n",
    "prev_running_average = 0.81 # dummy number\n",
    "running_average = (frac * prev_running_average) + (1 - frac) * g**2\n",
    "prev_running_average = running_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8b1e4688-1841-4357-b583-99b5c4af906c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.609999999999998"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_average # doesnt exead the squared g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b6aadd1a-d627-4f5b-94e6-a4cd2b15f264",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10000):\n",
    "    running_average = (frac * prev_running_average) + (1 - frac) * g**2\n",
    "    prev_running_average = running_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "441351ad-457f-4fe5-97a2-f66aabe20091",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunningSquaredAverage:\n",
    "    def __init__(self, frac=0.9):\n",
    "        self.frac = frac\n",
    "        self.prev_avg = 0\n",
    "\n",
    "    def __call__(self, g):\n",
    "        x = (self.frac * self.prev_avg) + (1 - self.frac) * g**2\n",
    "        self.prev_avg = x\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5021690f-3daa-476c-86b5-318c51107e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-15.999999949999994\n",
      "Avg:  15.999999999999991\n",
      "3.9999999999999987\n",
      "Expected: ~16\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "E = RunningSquaredAverage()\n",
    "g = 4\n",
    "smooth = 1e-8\n",
    "for _ in range(100000):\n",
    "    y = E.prev_avg\n",
    "    x = E(g)\n",
    "delta_update =(-math.sqrt(y)+smooth/math.sqrt(x)+smooth) * g\n",
    "print(delta_update)\n",
    "print(\"Avg: \", x)\n",
    "print(math.sqrt(x))\n",
    "print(f\"Expected: ~{g**2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3fa90b3a-2b17-4a20-aa2a-1417c84ed9c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "cb50e1df-0ca2-4a17-8a0e-c8dcdad74bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: x = 9.999683772234023, f(x) = 99.99367554468047, Delta LR = -1.5811388298865475e-05\n",
      "Iteration 100: x = 9.95898087303343, f(x) = 99.18130002944572, Delta LR = -2.337573191426562e-05\n",
      "Iteration 200: x = 9.907466927593864, f(x) = 98.1579009213662, Delta LR = -2.8288042081624868e-05\n",
      "Iteration 300: x = 9.847278303305446, f(x) = 96.96888998275018, Delta LR = -3.251561731469357e-05\n",
      "Iteration 400: x = 9.77964254514119, f(x) = 95.64140831073566, Delta LR = -3.6306066562581755e-05\n",
      "Iteration 500: x = 9.705426247862432, f(x) = 94.19529865269705, Delta LR = -3.979098305594291e-05\n",
      "Iteration 600: x = 9.62529125028693, f(x) = 92.64623165285012, Delta LR = -4.305039703934605e-05\n",
      "Iteration 700: x = 9.53976973276105, f(x) = 91.00720655410383, Delta LR = -4.613742518181055e-05\n",
      "Iteration 800: x = 9.44930551796149, f(x) = 89.28937477177746, Delta LR = -4.90895814629242e-05\n",
      "Iteration 900: x = 9.354278945963612, f(x) = 87.50253459889811, Delta LR = -5.193463585625118e-05\n",
      "Iteration 1000: x = 9.255022879959496, f(x) = 85.65544850857376, Delta LR = -5.469392607897328e-05\n",
      "Iteration 1100: x = 9.151833537794303, f(x) = 83.75605710349659, Delta LR = -5.738435824593075e-05\n",
      "Iteration 1200: x = 9.0449781199473, f(x) = 81.8116291903254, Delta LR = -6.001968060630373e-05\n",
      "Iteration 1300: x = 8.934700357915675, f(x) = 79.82887048573849, Delta LR = -6.261132986836247e-05\n",
      "Iteration 1400: x = 8.821224659472248, f(x) = 77.81400449288128, Delta LR = -6.516901393844561e-05\n",
      "Iteration 1500: x = 8.704759276303612, f(x) = 75.77283405839378, Delta LR = -6.770112556048572e-05\n",
      "Iteration 1600: x = 8.585498771734487, f(x) = 73.7107891594544, Delta LR = -7.021504381651398e-05\n",
      "Iteration 1700: x = 8.463625975554667, f(x) = 71.63296465408368, Delta LR = -7.271735914238697e-05\n",
      "Iteration 1800: x = 8.33931355533609, f(x) = 69.54415057421227, Delta LR = -7.521404491253401e-05\n",
      "Iteration 1900: x = 8.212725295884933, f(x) = 67.44885678566827, Delta LR = -7.771059092954355e-05\n",
      "Iteration 2000: x = 8.084017153095157, f(x) = 65.35133333153674, Delta LR = -8.021210928065948e-05\n",
      "Iteration 2100: x = 7.9533381310035836, f(x) = 63.25558742607558, Delta LR = -8.272341986238064e-05\n",
      "Iteration 2200: x = 7.820831018577391, f(x) = 61.16539782114228, Delta LR = -8.524912077573664e-05\n",
      "Iteration 2300: x = 7.686633013984029, f(x) = 59.084327091669195, Delta LR = -8.779364737249362e-05\n",
      "Iteration 2400: x = 7.550876257706806, f(x) = 57.015732259200334, Delta LR = -9.036132275108628e-05\n",
      "Iteration 2500: x = 7.413688291152253, f(x) = 54.962774078368014, Delta LR = -9.295640181330333e-05\n",
      "Iteration 2600: x = 7.275192453864943, f(x) = 52.92842524077341, Delta LR = -9.558311050458481e-05\n",
      "Iteration 2700: x = 7.135508229789834, f(x) = 50.91547769739845, Delta LR = -9.824568151089491e-05\n",
      "Iteration 2800: x = 6.994751550971721, f(x) = 48.92654925982129, Delta LR = -0.00010094838743282069\n",
      "Iteration 2900: x = 6.853035065493529, f(x) = 46.964089608883896, Delta LR = -0.00010369557227541875\n",
      "Iteration 3000: x = 6.710468375214038, f(x) = 45.03038581474773, Delta LR = -0.00010649168196170901\n",
      "Iteration 3100: x = 6.567158247887083, f(x) = 43.12756745279135, Delta LR = -0.00010934129448574483\n",
      "Iteration 3200: x = 6.423208807466648, f(x) = 41.257611384317116, Delta LR = -0.0001122491502590066\n",
      "Iteration 3300: x = 6.278721705779852, f(x) = 39.42234625863106, Delta LR = -0.00011522018316536032\n",
      "Iteration 3400: x = 6.133796278248714, f(x) = 37.623456783057776, Delta LR = -0.00011825955282087596\n",
      "Iteration 3500: x = 5.98852968593533, f(x) = 35.8624877993287, Delta LR = -0.0001213726785327348\n",
      "Iteration 3600: x = 5.843017045854989, f(x) = 34.14084819815196, Delta LR = -0.000124565275464697\n",
      "Iteration 3700: x = 5.697351551231608, f(x) = 32.45981469832121, Delta LR = -0.00012784339354448654\n",
      "Iteration 3800: x = 5.551624583148887, f(x) = 30.820535512223053, Delta LR = -0.00013121345969105668\n",
      "Iteration 3900: x = 5.405925814869293, f(x) = 29.224033915870226, Delta LR = -0.00013468232399779944\n",
      "Iteration 4000: x = 5.260343309944241, f(x) = 27.67121173847513, Delta LR = -0.0001382573105828371\n",
      "Iteration 4100: x = 5.114963615117536, f(x) = 26.162852783976252, Delta LR = -0.0001419462739117539\n",
      "Iteration 4200: x = 4.969871848925289, f(x) = 24.699626194740066, Delta LR = -0.00014575766151451135\n",
      "Iteration 4300: x = 4.8251517868165426, f(x) = 23.282089765818874, Delta LR = -0.00014970058416076558\n",
      "Iteration 4400: x = 4.680885943556162, f(x) = 21.910693216581663, Delta LR = -0.0001537848947314767\n",
      "Iteration 4500: x = 4.53715565362425, f(x) = 20.585781425214495, Delta LR = -0.00015802127723604464\n",
      "Iteration 4600: x = 4.394041150292036, f(x) = 19.30759763045976, Delta LR = -0.00016242134768148456\n",
      "Iteration 4700: x = 4.251621644032241, f(x) = 18.076286604003414, Delta LR = -0.00016699776881385657\n",
      "Iteration 4800: x = 4.109975400911959, f(x) = 16.891897796101418, Delta LR = -0.00017176438113564987\n",
      "Iteration 4900: x = 3.9691798216172014, f(x) = 15.754388456333158, Delta LR = -0.00017673635307313958\n",
      "Iteration 5000: x = 3.8293115217713387, f(x) = 14.663626730770726, Delta LR = -0.00018193035374677673\n",
      "Iteration 5100: x = 3.6904464142345383, f(x) = 13.619394736336561, Delta LR = -0.00018736475251363154\n",
      "Iteration 5200: x = 3.5526597941091618, f(x) = 12.621391612679751, Delta LR = -0.0001930598503403204\n",
      "Iteration 5300: x = 3.416026427228307, f(x) = 11.669236551522191, Delta LR = -0.00019903814917535465\n",
      "Iteration 5400: x = 3.280620642972841, f(x) = 10.762471803099537, Delta LR = -0.00020532466688392363\n",
      "Iteration 5500: x = 3.1465164323490824, f(x) = 9.900565659042798, Delta LR = -0.0002119473070683005\n",
      "Iteration 5600: x = 3.013787552367706, f(x) = 9.082915410806528, Delta LR = -0.00021893729533308803\n",
      "Iteration 5700: x = 2.8825076378987933, f(x) = 8.308850282544881, Delta LR = -0.00022632969641355708\n",
      "Iteration 5800: x = 2.7527503223435446, f(x) = 7.577634337162489, Delta LR = -0.00023416403026611774\n",
      "Iteration 5900: x = 2.624589368666617, f(x) = 6.888469354117832, Delta LR = -0.0002424850099933833\n",
      "Iteration 6000: x = 2.4980988125836543, f(x) = 6.240497677431864, Delta LR = -0.00025134343071473564\n",
      "Iteration 6100: x = 2.3733531200074083, f(x) = 5.632805032248899, Delta LR = -0.00026079724671371326\n",
      "Iteration 6200: x = 2.2504273612381267, f(x) = 5.064423308209198, Delta LR = -0.0002709128851208061\n",
      "Iteration 6300: x = 2.129397404859233, f(x) = 4.534333307821235, Delta LR = -0.00028176685905184526\n",
      "Iteration 6400: x = 2.01034013489373, f(x) = 4.041467457964542, Delta LR = -0.000293447762991467\n",
      "Iteration 6500: x = 1.8933336955250424, f(x) = 3.584712482610514, Delta LR = -0.0003060587604269103\n",
      "Iteration 6600: x = 1.7784577686350163, f(x) = 3.162912034818241, Delta LR = -0.00031972071144577475\n",
      "Iteration 6700: x = 1.6657938906254408, f(x) = 2.774869286045043, Delta LR = -0.00033457614090619096\n",
      "Iteration 6800: x = 1.5554258165557793, f(x) = 2.419349470808213, Delta LR = -0.0003507943229800158\n",
      "Iteration 6900: x = 1.4474399416717627, f(x) = 2.095082384746756, Delta LR = -0.0003685778663108848\n",
      "Iteration 7000: x = 1.3419257930911326, f(x) = 1.8007648341632652, Delta LR = -0.0003881713428909008\n",
      "Iteration 7100: x = 1.2389766080040978, f(x) = 1.53506303518134, Delta LR = -0.0004098727404956582\n",
      "Iteration 7200: x = 1.1386900196019636, f(x) = 1.2966149607411201, Delta LR = -0.0004340488779610464\n",
      "Iteration 7300: x = 1.0411688786104263, f(x) = 1.0840326337868926, Delta LR = -0.00046115647968171893\n",
      "Iteration 7400: x = 0.9465222475965706, f(x) = 0.8959043651952636, Delta LR = -0.0004917714889991337\n",
      "Iteration 7500: x = 0.8548666184147117, f(x) = 0.7307969352798043, Delta LR = -0.0005266306364490854\n",
      "Iteration 7600: x = 0.766327422278029, f(x) = 0.5872577181352885, Delta LR = -0.0005666916813077777\n",
      "Iteration 7700: x = 0.6810409302880639, f(x) = 0.4638167487276315, Delta LR = -0.0006132228933209856\n",
      "Iteration 7800: x = 0.5991566853674057, f(x) = 0.3589887336204564, Delta LR = -0.0006679397681936973\n",
      "Iteration 7900: x = 0.520840674075083, f(x) = 0.2712750077709869, Delta LR = -0.0007332208275188174\n",
      "Iteration 8000: x = 0.4462795562288286, f(x) = 0.19916544230780017, Delta LR = -0.0008124614780014578\n",
      "Iteration 8100: x = 0.37568645480569995, f(x) = 0.14114031232447524, Delta LR = -0.0009106810675403907\n",
      "Iteration 8200: x = 0.3093091347709578, f(x) = 0.09567214085275852, Delta LR = -0.0010356226172304186\n",
      "Iteration 8300: x = 0.24744200964418947, f(x) = 0.061227548136755156, Delta LR = -0.0011998833109752697\n",
      "Iteration 8400: x = 0.19044463864845387, f(x) = 0.03626916038994017, Delta LR = -0.0014254072572605072\n",
      "Iteration 8500: x = 0.1387720603191876, f(x) = 0.01925768472523224, Delta LR = -0.001754073130224337\n",
      "Iteration 8600: x = 0.09302891702487283, f(x) = 0.008654379402820673, Delta LR = -0.002276765067238723\n",
      "Iteration 8700: x = 0.054078468068754354, f(x) = 0.0029244807086632844, Delta LR = -0.0032334822309784852\n",
      "Iteration 8800: x = 0.023308506045864926, f(x) = 0.0005432864540901218, Delta LR = -0.005518567671502661\n",
      "Iteration 8900: x = 0.0035613405341738172, f(x) = 1.268314640034945e-05, Delta LR = -0.01677491439576593\n",
      "Iteration 9000: x = 1.53242536647403e-27, f(x) = 2.3483275038130654e-54, Delta LR = -0.6952638500809738\n",
      "Iteration 9100: x = 2.1372843876915763e-31, f(x) = 4.567984553870156e-62, Delta LR = -0.9999857927889872\n",
      "Iteration 9200: x = 2.136737894967377e-31, f(x) = 4.565648831789618e-62, Delta LR = -0.9999999996226285\n",
      "Iteration 9300: x = 2.1367378804535746e-31, f(x) = 4.565648769765234e-62, Delta LR = -0.99999999999999\n",
      "Iteration 9400: x = 2.1367378804531998e-31, f(x) = 4.565648769763633e-62, Delta LR = -1.0\n",
      "Iteration 9500: x = 2.1367378804531998e-31, f(x) = 4.565648769763633e-62, Delta LR = -1.0\n",
      "Iteration 9600: x = 2.1367378804531998e-31, f(x) = 4.565648769763633e-62, Delta LR = -1.0\n",
      "Iteration 9700: x = 2.1367378804531998e-31, f(x) = 4.565648769763633e-62, Delta LR = -1.0\n",
      "Iteration 9800: x = 2.1367378804531998e-31, f(x) = 4.565648769763633e-62, Delta LR = -1.0\n",
      "Iteration 9900: x = 2.1367378804531998e-31, f(x) = 4.565648769763633e-62, Delta LR = -1.0\n",
      "Final result:\n",
      "x = -2.1367378804531998e-31, f(x) = 4.565648769763633e-62\n"
     ]
    }
   ],
   "source": [
    "class RunningSquaredAverage:\n",
    "    def __init__(self, frac=0.9):\n",
    "        self.frac = frac\n",
    "        self.prev_avg = 0\n",
    "\n",
    "    def __call__(self, g):\n",
    "        x = self.frac * self.prev_avg + (1 - self.frac) * g**2\n",
    "        self.prev_avg = x\n",
    "        return x\n",
    "\n",
    "import math\n",
    "\n",
    "# Initialize running averages for squared gradients and squared updates\n",
    "E_g = RunningSquaredAverage(frac=0.9)\n",
    "E_delta_x = RunningSquaredAverage(frac=0.9)\n",
    "\n",
    "# Hyperparameters\n",
    "smooth = 1e-8\n",
    "num_iterations = 10000\n",
    "\n",
    "# Starting point\n",
    "x = 10.0\n",
    "\n",
    "# Function to optimize: f(x) = x^2\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "# Gradient of the function: f'(x) = 2x\n",
    "def grad_f(x):\n",
    "    return 2 * x\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    # Compute the gradient\n",
    "    g = grad_f(x)\n",
    "    \n",
    "    # Compute the running average of squared gradients\n",
    "    avg_g2 = E_g(g)\n",
    "\n",
    "    # Compute the update step (delta)\n",
    "    rms_delta_x_prev = math.sqrt(E_delta_x.prev_avg + smooth)\n",
    "    rms_g = math.sqrt(avg_g2 + smooth)\n",
    "    delta_learning_rate = - (rms_delta_x_prev / rms_g)\n",
    "    delta_update = delta_learning_rate * g\n",
    "\n",
    "    # Update the parameter\n",
    "    x += delta_update\n",
    "\n",
    "    # Compute the running average of squared updates\n",
    "    avg_delta_x2 = E_delta_x(delta_update)\n",
    "\n",
    "    # Print the progress every 100 iterations\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Iteration {i}: x = {x}, f(x) = {f(x)}, Delta LR = {delta_learning_rate}\")\n",
    "\n",
    "# Final result\n",
    "print(\"Final result:\")\n",
    "print(f\"x = {x}, f(x) = {f(x)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "25e28021-8865-4161-810b-e05efc00f5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST accuracy before training: 11.27%\n",
      "Training on MNIST...\n",
      "Epoch 1, Loss: 0.32213895300478695\n",
      "Epoch 2, Loss: 0.14255688821912796\n",
      "Epoch 3, Loss: 0.1044019699863184\n",
      "Epoch 4, Loss: 0.08856559921848352\n",
      "Epoch 5, Loss: 0.0748972068394302\n",
      "MNIST accuracy before training on Fashion-MNIST: 97.07%\n",
      "Training on Fashion-MNIST...\n",
      "Epoch 1, Loss: 0.4951994276440728\n",
      "Fashion-MNIST accuracy EPOCH 0: 85.86%\n",
      "MNIST accuracy EPOCH 0: 17.08%\n",
      "Epoch 1, Loss: 0.35382833646368117\n",
      "Fashion-MNIST accuracy EPOCH 1: 86.26%\n",
      "MNIST accuracy EPOCH 1: 17.47%\n",
      "Epoch 1, Loss: 0.31791195423522994\n",
      "Fashion-MNIST accuracy EPOCH 2: 86.70%\n",
      "MNIST accuracy EPOCH 2: 22.83%\n",
      "Epoch 1, Loss: 0.29418867187840597\n",
      "Fashion-MNIST accuracy EPOCH 3: 87.39%\n",
      "MNIST accuracy EPOCH 3: 16.77%\n",
      "Epoch 1, Loss: 0.27839763144027196\n",
      "Fashion-MNIST accuracy EPOCH 4: 87.34%\n",
      "MNIST accuracy EPOCH 4: 16.61%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "mnist_trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "mnist_trainloader = torch.utils.data.DataLoader(mnist_trainset, batch_size=64, shuffle=True)\n",
    "mnist_testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "mnist_testloader = torch.utils.data.DataLoader(mnist_testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Load Fashion-MNIST dataset\n",
    "fashionmnist_trainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "fashionmnist_trainloader = torch.utils.data.DataLoader(fashionmnist_trainset, batch_size=64, shuffle=True)\n",
    "fashionmnist_testset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "fashionmnist_testloader = torch.utils.data.DataLoader(fashionmnist_testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize the network, loss function and optimizer\n",
    "net = SimpleNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# Train the network on MNIST\n",
    "def train(net, trainloader, epochs=5):\n",
    "    net.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in trainloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(trainloader)}\")\n",
    "\n",
    "def test(net, testloader):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "# Train on MNIST\n",
    "init_accuracy = test(net, mnist_testloader)\n",
    "print(f\"MNIST accuracy before training: {init_accuracy*100:.2f}%\")\n",
    "print(\"Training on MNIST...\")\n",
    "train(net, mnist_trainloader)\n",
    "mnist_accuracy_before = test(net, mnist_testloader)\n",
    "print(f\"MNIST accuracy before training on Fashion-MNIST: {mnist_accuracy_before*100:.2f}%\")\n",
    "\n",
    "# Train on Fashion-MNIST\n",
    "print(\"Training on Fashion-MNIST...\")\n",
    "for i in range(5):\n",
    "    train(net, fashionmnist_trainloader, epochs=1)\n",
    "    fashion_mnist_accuracy = test(net, fashionmnist_testloader)\n",
    "    mnist_accuracy_after = test(net, mnist_testloader)\n",
    "    \n",
    "    print(f\"Fashion-MNIST accuracy EPOCH {i}: {fashion_mnist_accuracy*100:.2f}%\")\n",
    "    print(f\"MNIST accuracy EPOCH {i}: {mnist_accuracy_after*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f838850e-5105-4b65-88d7-c1e5a08a4184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on MNIST with novelty-based weighting...\n",
      "Epoch 1, Weighted Loss: 1.1457005473755315\n",
      "Epoch 2, Weighted Loss: 0.6510715897498863\n",
      "Epoch 3, Weighted Loss: 0.6317948544902334\n",
      "Epoch 4, Weighted Loss: 0.4452814559327133\n",
      "Epoch 5, Weighted Loss: 0.3825417345028315\n",
      "MNIST accuracy before training on Fashion-MNIST: 94.89%\n",
      "Training on Fashion-MNIST with novelty-based weighting...\n",
      "Epoch 1, Weighted Loss: 2.0473402260081857\n",
      "Epoch 2, Weighted Loss: 1.0485144116794631\n",
      "Epoch 3, Weighted Loss: 0.93437825934465\n",
      "Epoch 4, Weighted Loss: 0.8652006075072136\n",
      "Epoch 5, Weighted Loss: 0.8258634173730289\n",
      "Fashion-MNIST accuracy: 85.90%\n",
      "MNIST accuracy after training on Fashion-MNIST: 12.22%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "mnist_trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "mnist_trainloader = torch.utils.data.DataLoader(mnist_trainset, batch_size=64, shuffle=True)\n",
    "mnist_testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "mnist_testloader = torch.utils.data.DataLoader(mnist_testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Load Fashion-MNIST dataset\n",
    "fashionmnist_trainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "fashionmnist_trainloader = torch.utils.data.DataLoader(fashionmnist_trainset, batch_size=64, shuffle=True)\n",
    "fashionmnist_testset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "fashionmnist_testloader = torch.utils.data.DataLoader(fashionmnist_testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize the network, loss function and optimizer\n",
    "net = SimpleNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')  # Set reduction to 'none' to get individual losses\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# Train the network with novelty-based weighting\n",
    "def train_with_novelty(net, trainloader, epochs=5, novelty_threshold=2.0):\n",
    "    net.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in trainloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            losses = criterion(outputs, labels)\n",
    "\n",
    "            # Calculate novelty (weight) for each input\n",
    "            novelty_weights = torch.exp(losses / novelty_threshold)\n",
    "            weighted_loss = (losses * novelty_weights).mean()\n",
    "\n",
    "            weighted_loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += weighted_loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Weighted Loss: {running_loss/len(trainloader)}\")\n",
    "\n",
    "def test(net, testloader):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "# Train on MNIST with novelty-based weighting\n",
    "print(\"Training on MNIST with novelty-based weighting...\")\n",
    "train_with_novelty(net, mnist_trainloader)\n",
    "mnist_accuracy_before = test(net, mnist_testloader)\n",
    "print(f\"MNIST accuracy before training on Fashion-MNIST: {mnist_accuracy_before*100:.2f}%\")\n",
    "\n",
    "# Train on Fashion-MNIST with novelty-based weighting\n",
    "print(\"Training on Fashion-MNIST with novelty-based weighting...\")\n",
    "train_with_novelty(net, fashionmnist_trainloader)\n",
    "fashion_mnist_accuracy = test(net, fashionmnist_testloader)\n",
    "mnist_accuracy_after = test(net, mnist_testloader)\n",
    "\n",
    "print(f\"Fashion-MNIST accuracy: {fashion_mnist_accuracy*100:.2f}%\")\n",
    "print(f\"MNIST accuracy after training on Fashion-MNIST: {mnist_accuracy_after*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fc397161-c5f0-4851-8ed4-de1c7e9f51f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on MNIST with novelty-based weighting...\n",
      "Epoch trigger\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "array must not contain infs or NaNs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[123], line 98\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Train on MNIST with novelty-based weighting\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining on MNIST with novelty-based weighting...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[43mtrain_with_novelty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmnist_trainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m mnist_accuracy_before \u001b[38;5;241m=\u001b[39m test(net, mnist_testloader)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMNIST accuracy before training on Fashion-MNIST: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmnist_accuracy_before\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[123], line 69\u001b[0m, in \u001b[0;36mtrain_with_novelty\u001b[0;34m(net, trainloader, memory_buffer, epochs, novelty_threshold)\u001b[0m\n\u001b[1;32m     66\u001b[0m losses \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Calculate novelty for each input\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m novelty_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([calculate_novelty(f, memory_buffer) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features])\n\u001b[1;32m     70\u001b[0m weighted_loss \u001b[38;5;241m=\u001b[39m (losses \u001b[38;5;241m*\u001b[39m novelty_weights\u001b[38;5;241m.\u001b[39mto(device))\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     72\u001b[0m weighted_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[123], line 69\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     66\u001b[0m losses \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Calculate novelty for each input\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m novelty_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[43mcalculate_novelty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_buffer\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features])\n\u001b[1;32m     70\u001b[0m weighted_loss \u001b[38;5;241m=\u001b[39m (losses \u001b[38;5;241m*\u001b[39m novelty_weights\u001b[38;5;241m.\u001b[39mto(device))\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     72\u001b[0m weighted_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[123], line 52\u001b[0m, in \u001b[0;36mcalculate_novelty\u001b[0;34m(feature, memory_buffer, threshold)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m memory_buffer:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1.0\u001b[39m  \u001b[38;5;66;03m# High novelty for the first few inputs\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m distances \u001b[38;5;241m=\u001b[39m [euclidean(feature\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(), mem) \u001b[38;5;28;01mfor\u001b[39;00m mem \u001b[38;5;129;01min\u001b[39;00m memory_buffer]\n\u001b[1;32m     53\u001b[0m min_distance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(distances)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mexp(torch\u001b[38;5;241m.\u001b[39mtensor(min_distance \u001b[38;5;241m/\u001b[39m threshold))\n",
      "Cell \u001b[0;32mIn[123], line 52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m memory_buffer:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1.0\u001b[39m  \u001b[38;5;66;03m# High novelty for the first few inputs\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m distances \u001b[38;5;241m=\u001b[39m [\u001b[43meuclidean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmem\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m mem \u001b[38;5;129;01min\u001b[39;00m memory_buffer]\n\u001b[1;32m     53\u001b[0m min_distance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(distances)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mexp(torch\u001b[38;5;241m.\u001b[39mtensor(min_distance \u001b[38;5;241m/\u001b[39m threshold))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ptb/lib/python3.10/site-packages/scipy/spatial/distance.py:520\u001b[0m, in \u001b[0;36meuclidean\u001b[0;34m(u, v, w)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meuclidean\u001b[39m(u, v, w\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    485\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;124;03m    Computes the Euclidean distance between two 1-D arrays.\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    518\u001b[0m \n\u001b[1;32m    519\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mminkowski\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ptb/lib/python3.10/site-packages/scipy/spatial/distance.py:480\u001b[0m, in \u001b[0;36mminkowski\u001b[0;34m(u, v, p, w)\u001b[0m\n\u001b[1;32m    478\u001b[0m         root_w \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpower(w, \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mp)\n\u001b[1;32m    479\u001b[0m     u_v \u001b[38;5;241m=\u001b[39m root_w \u001b[38;5;241m*\u001b[39m u_v\n\u001b[0;32m--> 480\u001b[0m dist \u001b[38;5;241m=\u001b[39m \u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mord\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dist\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ptb/lib/python3.10/site-packages/scipy/linalg/_misc.py:146\u001b[0m, in \u001b[0;36mnorm\u001b[0;34m(a, ord, axis, keepdims, check_finite)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# Differs from numpy only in non-finite handling and the use of blas.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_finite:\n\u001b[0;32m--> 146\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray_chkfinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(a)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ptb/lib/python3.10/site-packages/numpy/lib/function_base.py:630\u001b[0m, in \u001b[0;36masarray_chkfinite\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    628\u001b[0m a \u001b[38;5;241m=\u001b[39m asarray(a, dtype\u001b[38;5;241m=\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39morder)\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m a\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mchar \u001b[38;5;129;01min\u001b[39;00m typecodes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllFloat\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(a)\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m--> 630\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray must not contain infs or NaNs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[0;31mValueError\u001b[0m: array must not contain infs or NaNs"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        feature = self.relu(self.fc2(x))\n",
    "        output = self.fc3(feature)\n",
    "        return feature, output\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "mnist_trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "mnist_trainloader = torch.utils.data.DataLoader(mnist_trainset, batch_size=64, shuffle=True)\n",
    "mnist_testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "mnist_testloader = torch.utils.data.DataLoader(mnist_testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Load Fashion-MNIST dataset\n",
    "fashionmnist_trainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "fashionmnist_trainloader = torch.utils.data.DataLoader(fashionmnist_trainset, batch_size=64, shuffle=True)\n",
    "fashionmnist_testset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "fashionmnist_testloader = torch.utils.data.DataLoader(fashionmnist_testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize the network, loss function and optimizer\n",
    "net = SimpleNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')  # Set reduction to 'none' to get individual losses\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# Memory buffer to store past inputs' features\n",
    "memory_buffer = []\n",
    "\n",
    "# Function to calculate novelty based on feature distance\n",
    "def calculate_novelty(feature, memory_buffer, threshold=0.1):\n",
    "    if not memory_buffer:\n",
    "        return 1.0  # High novelty for the first few inputs\n",
    "    distances = [euclidean(feature.cpu().detach().numpy(), mem) for mem in memory_buffer]\n",
    "    min_distance = min(distances)\n",
    "    return torch.exp(torch.tensor(min_distance / threshold))\n",
    "\n",
    "# Train the network with novelty-based weighting\n",
    "def train_with_novelty(net, trainloader, memory_buffer, epochs=5, novelty_threshold=2.0):\n",
    "    net.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        print(\"Epoch trigger\")\n",
    "        for inputs, labels in trainloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            features, outputs = net(inputs)\n",
    "            losses = criterion(outputs, labels)\n",
    "\n",
    "            # Calculate novelty for each input\n",
    "            novelty_weights = torch.tensor([calculate_novelty(f, memory_buffer) for f in features])\n",
    "            weighted_loss = (losses * novelty_weights.to(device)).mean()\n",
    "\n",
    "            weighted_loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += weighted_loss.item()\n",
    "            \n",
    "            # Update memory buffer with features\n",
    "            memory_buffer.extend(features.cpu().detach().numpy())\n",
    "            if len(memory_buffer) > 1000:  # Keep the memory buffer size manageable\n",
    "                memory_buffer = memory_buffer[-1000:]\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Weighted Loss: {running_loss/len(trainloader)}\")\n",
    "\n",
    "def test(net, testloader):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            _, outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "# Train on MNIST with novelty-based weighting\n",
    "print(\"Training on MNIST with novelty-based weighting...\")\n",
    "train_with_novelty(net, mnist_trainloader, memory_buffer=memory_buffer)\n",
    "mnist_accuracy_before = test(net, mnist_testloader)\n",
    "print(f\"MNIST accuracy before training on Fashion-MNIST: {mnist_accuracy_before*100:.2f}%\")\n",
    "\n",
    "# Train on Fashion-MNIST with novelty-based weighting\n",
    "print(\"Training on Fashion-MNIST with novelty-based weighting...\")\n",
    "train_with_novelty(net, fashionmnist_trainloader, memory_buffer=memory_buffer)\n",
    "fashion_mnist_accuracy = test(net, fashionmnist_testloader)\n",
    "mnist_accuracy_after = test(net, mnist_testloader)\n",
    "\n",
    "print(f\"Fashion-MNIST accuracy: {fashion_mnist_accuracy*100:.2f}%\")\n",
    "print(f\"MNIST accuracy after training on Fashion-MNIST: {mnist_accuracy_after*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "16d269d2-f92a-4562-9c3e-cd68c938d14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on MNIST with novelty-based weighting...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "array must not contain infs or NaNs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[130], line 101\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Train on MNIST with novelty-based weighting\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining on MNIST with novelty-based weighting...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 101\u001b[0m \u001b[43mtrain_with_novelty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmnist_trainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m mnist_accuracy_before \u001b[38;5;241m=\u001b[39m test(net, mnist_testloader)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMNIST accuracy before training on Fashion-MNIST: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmnist_accuracy_before\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[130], line 69\u001b[0m, in \u001b[0;36mtrain_with_novelty\u001b[0;34m(net, trainloader, memory_buffer, epochs, novelty_threshold)\u001b[0m\n\u001b[1;32m     66\u001b[0m losses \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Calculate novelty for each input\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m novelty_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([calculate_novelty(f\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(), memory_buffer) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features])\n\u001b[1;32m     70\u001b[0m weighted_loss \u001b[38;5;241m=\u001b[39m (losses \u001b[38;5;241m*\u001b[39m novelty_weights\u001b[38;5;241m.\u001b[39mto(device))\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     72\u001b[0m weighted_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[130], line 69\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     66\u001b[0m losses \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Calculate novelty for each input\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m novelty_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[43mcalculate_novelty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_buffer\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features])\n\u001b[1;32m     70\u001b[0m weighted_loss \u001b[38;5;241m=\u001b[39m (losses \u001b[38;5;241m*\u001b[39m novelty_weights\u001b[38;5;241m.\u001b[39mto(device))\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     72\u001b[0m weighted_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[130], line 53\u001b[0m, in \u001b[0;36mcalculate_novelty\u001b[0;34m(feature, memory_buffer, threshold)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m memory_buffer:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1.0\u001b[39m  \u001b[38;5;66;03m# High novelty for the first few inputs\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m distances \u001b[38;5;241m=\u001b[39m [euclidean(feature, mem) \u001b[38;5;28;01mfor\u001b[39;00m mem \u001b[38;5;129;01min\u001b[39;00m memory_buffer]\n\u001b[1;32m     54\u001b[0m min_distance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(distances)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mexp(torch\u001b[38;5;241m.\u001b[39mtensor(min_distance \u001b[38;5;241m/\u001b[39m threshold))\n",
      "Cell \u001b[0;32mIn[130], line 53\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m memory_buffer:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1.0\u001b[39m  \u001b[38;5;66;03m# High novelty for the first few inputs\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m distances \u001b[38;5;241m=\u001b[39m [\u001b[43meuclidean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmem\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m mem \u001b[38;5;129;01min\u001b[39;00m memory_buffer]\n\u001b[1;32m     54\u001b[0m min_distance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(distances)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mexp(torch\u001b[38;5;241m.\u001b[39mtensor(min_distance \u001b[38;5;241m/\u001b[39m threshold))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ptb/lib/python3.10/site-packages/scipy/spatial/distance.py:520\u001b[0m, in \u001b[0;36meuclidean\u001b[0;34m(u, v, w)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meuclidean\u001b[39m(u, v, w\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    485\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;124;03m    Computes the Euclidean distance between two 1-D arrays.\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    518\u001b[0m \n\u001b[1;32m    519\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mminkowski\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ptb/lib/python3.10/site-packages/scipy/spatial/distance.py:480\u001b[0m, in \u001b[0;36mminkowski\u001b[0;34m(u, v, p, w)\u001b[0m\n\u001b[1;32m    478\u001b[0m         root_w \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpower(w, \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mp)\n\u001b[1;32m    479\u001b[0m     u_v \u001b[38;5;241m=\u001b[39m root_w \u001b[38;5;241m*\u001b[39m u_v\n\u001b[0;32m--> 480\u001b[0m dist \u001b[38;5;241m=\u001b[39m \u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mord\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dist\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ptb/lib/python3.10/site-packages/scipy/linalg/_misc.py:146\u001b[0m, in \u001b[0;36mnorm\u001b[0;34m(a, ord, axis, keepdims, check_finite)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# Differs from numpy only in non-finite handling and the use of blas.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_finite:\n\u001b[0;32m--> 146\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray_chkfinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(a)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ptb/lib/python3.10/site-packages/numpy/lib/function_base.py:630\u001b[0m, in \u001b[0;36masarray_chkfinite\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    628\u001b[0m a \u001b[38;5;241m=\u001b[39m asarray(a, dtype\u001b[38;5;241m=\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39morder)\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m a\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mchar \u001b[38;5;129;01min\u001b[39;00m typecodes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllFloat\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(a)\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m--> 630\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray must not contain infs or NaNs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[0;31mValueError\u001b[0m: array must not contain infs or NaNs"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from scipy.spatial.distance import euclidean\n",
    "import numpy as np\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        feature = self.relu(self.fc2(x))\n",
    "        output = self.fc3(feature)\n",
    "        return feature, output\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "mnist_trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "mnist_trainloader = torch.utils.data.DataLoader(mnist_trainset, batch_size=64, shuffle=True)\n",
    "mnist_testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "mnist_testloader = torch.utils.data.DataLoader(mnist_testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Load Fashion-MNIST dataset\n",
    "fashionmnist_trainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "fashionmnist_trainloader = torch.utils.data.DataLoader(fashionmnist_trainset, batch_size=64, shuffle=True)\n",
    "fashionmnist_testset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "fashionmnist_testloader = torch.utils.data.DataLoader(fashionmnist_testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize the network, loss function and optimizer\n",
    "net = SimpleNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')  # Set reduction to 'none' to get individual losses\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# Memory buffer to store past inputs' features\n",
    "memory_buffer = []\n",
    "\n",
    "# Function to calculate novelty based on feature distance\n",
    "def calculate_novelty(feature, memory_buffer, threshold=0.1):\n",
    "    if not memory_buffer:\n",
    "        return 1.0  # High novelty for the first few inputs\n",
    "    distances = [euclidean(feature, mem) for mem in memory_buffer]\n",
    "    min_distance = min(distances)\n",
    "    return torch.exp(torch.tensor(min_distance / threshold))\n",
    "\n",
    "# Train the network with novelty-based weighting\n",
    "def train_with_novelty(net, trainloader, memory_buffer, epochs=5, novelty_threshold=2.0):\n",
    "    net.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in trainloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            features, outputs = net(inputs)\n",
    "            losses = criterion(outputs, labels)\n",
    "\n",
    "            # Calculate novelty for each input\n",
    "            novelty_weights = torch.tensor([calculate_novelty(f.cpu().detach().numpy(), memory_buffer) for f in features])\n",
    "            weighted_loss = (losses * novelty_weights.to(device)).mean()\n",
    "\n",
    "            weighted_loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += weighted_loss.item()\n",
    "            \n",
    "            # Update memory buffer with features\n",
    "            features_np = features.cpu().detach().numpy()\n",
    "            for feature in features_np:\n",
    "                if np.isfinite(feature).all():\n",
    "                    memory_buffer.append(feature)\n",
    "            if len(memory_buffer) > 1000:  # Keep the memory buffer size manageable\n",
    "                memory_buffer = memory_buffer[-1000:]\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Weighted Loss: {running_loss/len(trainloader)}\")\n",
    "\n",
    "def test(net, testloader):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            _, outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "# Train on MNIST with novelty-based weighting\n",
    "print(\"Training on MNIST with novelty-based weighting...\")\n",
    "train_with_novelty(net, mnist_trainloader, memory_buffer)\n",
    "mnist_accuracy_before = test(net, mnist_testloader)\n",
    "print(f\"MNIST accuracy before training on Fashion-MNIST: {mnist_accuracy_before*100:.2f}%\")\n",
    "\n",
    "# Train on Fashion-MNIST with novelty-based weighting\n",
    "print(\"Training on Fashion-MNIST with novelty-based weighting...\")\n",
    "train_with_novelty(net, fashionmnist_trainloader, memory_buffer)\n",
    "fashion_mnist_accuracy = test(net, fashionmnist_testloader)\n",
    "mnist_accuracy_after = test(net, mnist_testloader)\n",
    "\n",
    "print(f\"Fashion-MNIST accuracy: {fashion_mnist_accuracy*100:.2f}%\")\n",
    "print(f\"MNIST accuracy after training on Fashion-MNIST: {mnist_accuracy_after*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d2d5989e-1c80-41ef-8c9d-d7deb9cd44d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "numbers = [1, 2, 3, 4]\n",
    "\n",
    "# get sum\n",
    "total_sum = reduce(lambda total, currentNumber: total + currentNumber, numbers, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "070aef89-f342-42e5-9115-e6b3676fb676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f7dc55-158f-453e-b9c3-4bd09d0ab7f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
